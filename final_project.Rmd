---
title: "Cardiovascular Disease Data Analysis and Prediction"
author: Giovanni Billo, Muhammad Mubashar Shahzad, Carlos Velázquez Fernández, Fabio Vicig
date: "2025-01-07"
output: 
  html_document:
    toc: true
    toc_depth: '4'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 4
editor_options: 
  markdown: 
    wrap: 72
    
references:
  
  - id: narloch1995influence
    title: "Influence of breathing technique on arterial blood pressure during heavy weight lifting"
    author:
      - family: "Narloch"
        given: "Joseph A"
      - family: "Brandstater"
        given: "Murray E"
    container-title: "Archives of physical medicine and rehabilitation"
    volume: 76
    issue: 5
    page: "457-462"
    issued:
      year: 1995
    publisher: "Elsevier"
    
  - id: cardiovascular_disease_dataset
    author:
      - family: "Ulianova"
        given: "Svetlana"
    title: "Cardiovascular Disease Dataset"
    URL: "https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset"
    accessed:
      year: 2025
      month: 1
      day: 9
    type: dataset
    
  - id: malone2010pulse
    title: "Pulse Pressure. Why is it Important?"
    author:
      - family: "Malone"
        given: "AF"
      - family: "Reddan"
        given: "DN"
    container-title: "Peritoneal Dialysis International"
    volume: 30
    issue: 3
    page: "265-268"
    issued:
      year: 2010
    DOI: "10.3747/pdi.2010.00002"
    
  - id: tibshirani2009elements
    title: "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"
    author:
      - family: "Hastie"
        given: "Trevor"
      - family: "Tibshirani"
        given: "Robert"
      - family: "Friedman"
        given: "Jerome"
    container-title: "Springer Series in Statistics"
    edition: 2
    publisher: "Springer"
    issued:
      year: 2009
    DOI: "10.1007/978-0-387-84858-7"
  
  - id: grigoletto2014modello
    title: "Modello Lineare: Teoria e Applicazioni con R"
    author:
      - family: "Grigoletto"
        given: "Matteo"
      - family: "Pauli"
        given: "Francesco"
      - family: "Ventura"
        given: "Laura"
    container-title: "Statistica e Probabilità"
    publisher: "EGEA"
    issued:
      year: 2014
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

The analysis is centered around the prediction of presence of cardiovascular disease, given data about patients [@cardiovascular_disease_dataset].
The data includes different categorical and numerical variables commonly associated to an individual's health status.
After a brief data exploration, we perform first cluster analysis on the dataset and then fit different types of models (Trees and Random Forest, Logistic regression models and natural splines). All of these deliver a similar performance, but some are more explainable than others. 
Finally, we discuss some issues with the data, considering its unknown origin and unspecified gathering methods, and hint at how this work could possibly be improved. 

## Importing libraries

```{r package_installation}
packages <- c("arm", "caret", "cluster", "corrplot", "dendextend", "doParallel", "dplyr", 
              "effects", "GGally", "ggplot2", "glmnet", "gridExtra", "MASS", "patchwork", "parallel", 
              "pROC", "ranger", "reshape2", "rpart", "rpart.plot", "splines")

install_if_missing <- function(pkg) {
  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
  }
}

suppressMessages(invisible(lapply(packages, install_if_missing)))
```

```{r libraries}
suppressMessages(library(arm))
suppressMessages(library(caret))
suppressMessages(library(cluster))
suppressMessages(library(corrplot))
suppressMessages(library(dendextend))
suppressMessages(library(doParallel))
suppressMessages(library(dplyr))
suppressMessages(library(effects))
suppressMessages(library(GGally))
suppressMessages(library(ggplot2))
suppressMessages(library(glmnet))
suppressMessages(library(gridExtra))
suppressMessages(library(MASS))
suppressMessages(library(parallel))
suppressMessages(library(patchwork))
suppressMessages(library(pROC))
suppressMessages(library(ranger))
suppressMessages(library(reshape2))
suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
suppressMessages(library(splines))
```

## Data exploration

Firstly, an exploratory analysis has been conducted, with the purpose of identifying problems and have at a glance an idea of meaning and distribution of the variables.

The dataset contains 70000 observations of 12 variables. The source doesn't specify how the data was recovered or generated, and it has not been possible to find it elsewhere than Kaggle. The variables are the following:\

- **cardio** : objective variable of our analysis. It is a binary variable representing whether cardiovascular disease is present in the patient. \
- **age** : age of the patient, in days.\
- **gender** : categorical variable which is 1 for females and 2 for males.\
- **height** : height of the patient, in cm.\
- **weight** : weight of the patient, in kg.\
- **ap_hi** : systolic blood pressure.\
- **ap_lo** : diastolic blood pressure.\
- **cholesterol** : categorical variable with three levels of cholesterol: 1: normal, 2: above normal, 3: well above normal.\
- **gluc** : categorical variable with three levels of glucose: 1: normal, 2: above normal, 3: well above normal.\
- **smoke** : binary variable, representing smokers.\
- **alco** : binary variable, representing alcohol intake.\
- **active** : binary variable, representing physical activity.

The last three variables are marked as "subjective", meaning that this piece of information is likely supplied by patients themselves, possibly using different metrics.


### Preprocessing

First of all, we need to check if there are NA values in the data:
```{r 'Explorative analysis and data cleaning 1'}
cardio_with_id <- read.csv("data/cardio_train.csv", sep= ";", header=TRUE)
cardio <- cardio_with_id[,-1]
str(cardio)
sum(as.numeric(is.na(cardio)))
```

It seems like there are no NA values. Let's check some other general metrics:

```{r 'Explorative analysis and data cleaning 2'}
summary(cardio)
```

From the summary we can see that there are variables with impossible values: specifically, `ap_hi` and `ap_lo` have in some cases negative values, which doesn't make sense (even for a dead person), and in other cases extremely high values. 

A cut-off value value for the blood pressure is selected (generously) by basing on a study that measured blood pressure during intense exercise, and found a maximum pressure of 370/360 while using the Valsalva and a maximum of 250/230 without it 
[@narloch1995influence]. 

```{r}
summary(cardio[(cardio$ap_hi<0|cardio$ap_hi>400|cardio$ap_lo<0|cardio$ap_lo>400|cardio$ap_hi<cardio$ap_lo),])
cardio <- cardio[!(cardio$ap_hi < 25 | cardio$ap_hi > 400 | cardio$ap_lo <  10| cardio$ap_lo > 400 | cardio$ap_hi < (cardio$ap_lo + 1) | cardio$ap_hi-cardio$ap_lo > 160),]
```

1274 observations have completely unrealistic values for either systolic or diastolic pressure (or both). In 274 of them, the systolic pressure is less than the diastolic, which is also not possible. These wrong metrics are less than 2% of the dataset, so the most straightforward approach is just to eliminate the problematic rows.

In order to make data more understandable, we will transform the age (currently in days) to years.

```{r}
cardio$age = cardio$age/365
summary(cardio$age)
ggplot(cardio, aes(y = age)) +
  geom_boxplot(fill = "skyblue", color = "black") + labs(
    y = "Age"                                       
  ) +
  theme_minimal() 
```
```{r age_categorizing 1}
nrow(cardio[cardio$age<39,])
```
Given that there are only 4 observations concerning people < 39 years old, and the youngest one in the sample is 29, we will split data between under (or equal to) and over 55.

```{r age_categorizing 2}
cardio$age_cat <- ifelse(cardio$age <= 55, '30-55', '55-65')
table(cardio$age_cat)
```

Factorization must also be applied to some variables:
```{r factor_transformation}
# saving a copy of cardio without factor columns later on
cardio_cl <- subset(cardio, select = -age_cat)

cardio$gender <- factor(cardio$gender)
levels(cardio$gender) = c("F", "M")

cardio$cardio <- as.factor(cardio$cardio)

cardio$cholesterol <- factor(cardio$cholesterol)
levels(cardio$cholesterol) = c("normal", "above normal", "well above normal")

cardio$gluc <- factor(cardio$gluc)
levels(cardio$gluc) = c("normal", "above normal", "well above normal")

cardio$smoke <- factor(cardio$smoke)
levels(cardio$smoke) = c("No", "Yes")

cardio$alco <- factor(cardio$alco)
levels(cardio$alco) = c("No", "Yes")

cardio$active <- factor(cardio$active)
levels(cardio$active) = c("No", "Yes")

cardio$age_cat <- factor(cardio$age_cat)
levels(cardio$age_cat) = c("Under 55", "Over 55")
```


### Distribution

It is important to know the proportion of classes in the target variable.
```{r}
table(cardio$cardio)
```

It looks like the dataset is balanced with respect to `cardio`.

For convenience, numerical and categorical variables will be specified:
```{r}
numerical <- cardio[, c("age", "height", "weight", "ap_hi", "ap_lo")]
categorical <- cardio[!(names(cardio) %in% c("age", "height", "weight", "ap_hi", "ap_lo"))]
```

Let's explore first the frequency of the **numerical** variables in a more visual way:
```{r eval=TRUE}
num_vars <- ncol(numerical)
par(mfrow = c(ceiling(sqrt(num_vars)), floor(sqrt(num_vars)))) 
for (i in 1:num_vars) {
    hist(
        numerical[[i]], col = "skyblue",main=colnames(numerical)[i], xlab = "Value", ylab = "Frequency", border = "black"   )
}

```

We now proceed to plot the distribution of the numerical variables, also with respect to the disease status:

```{r pairs, , eval=FALSE}
set.seed(123)
custom_colors <- c("royalblue", "lightcoral")

plot.new()
pairs(
  numerical,
  labels = colnames(numerical),
  pch = 'o',
  bg = custom_colors[as.numeric(as.character(cardio$cardio)) + 1],
  col = custom_colors[as.numeric(as.character(cardio$cardio)) + 1],
  main = "\n",
  row1attop = TRUE,
  gap = 1,
  cex.labels = 1,
  cex = 0.8
)
legend(
  "topleft",
  legend = c("No disease", "Disease"),
  col = custom_colors,
  pch = 'o',
  pt.bg = custom_colors,
  bty = "n",
  xpd = TRUE,  # Allow drawing outside the plot area
  inset = c(0.8, -0.17)
)
```

```{r}

gdens1<- ggplot(cardio, aes(x = ap_hi, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 4) +  
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) +
  labs(fill = "Cardiovascular disease", x = "Systolic pressure") +  
  theme_minimal() +  
  theme(legend.position = "top")
gdens2<-ggplot(cardio, aes(x = ap_lo, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 4) +  
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Diastolic pressure") +  
  theme_minimal() +  
  theme(legend.position = "top")
gdens3<-ggplot(cardio, aes(x = ap_hi-ap_lo, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 4) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Pulse pressure") + 
  theme_minimal() +
  theme(legend.position = "top")
gdens4<-ggplot(cardio, aes(x = height, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 2) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Height") + 
  theme_minimal() +
  theme(legend.position = "top")
gdens5<-ggplot(cardio, aes(x = weight, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 2) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Weight") + 
  theme_minimal() +
  theme(legend.position = "top")
gdens6<-ggplot(cardio, aes(x = age, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 1) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Age") + 
  theme_minimal() +
  theme(legend.position = "top")
grid.arrange(gdens1,gdens2, nrow = 2)
grid.arrange(gdens3,gdens6, nrow = 2)
grid.arrange(gdens4,gdens5, nrow = 2)
```

```{r}
if (!dir.exists("images")) {
  dir.create("images")
}

save_plots <- function(plot_list){
  # Save each plot to the "images" directory using png() device
  for (plot_name in names(plot_list)) {
    # Open a PNG device
    png(filename = paste0("images/", plot_name, ".png"), width = 800, height = 600, res = 300)
    
    # Print the plot
    print(plot_list[[plot_name]])
    
    # Close the device
    dev.off()
  }
}
plots_density <- list(
  gdens1 = gdens1,
  gdens2 = gdens2,
  gdens3 = gdens3,
  gdens4 = gdens4,
  gdens5 = gdens5,
  gdens6 = gdens6
)
save_plots(plots_density)
```


Now let's explore the possible relationships of the **categorical** variables with respect to `cardio`.

```{r barplots}

stacked_bplot <- function(data, x, fill, fill_colors, title, x_label, y_label, fill_label) {
  data <- aggregate(x = list(count = rep(1, nrow(data))), by = list(cardio = data$cardio, group = data[[fill]]), FUN = sum)
  data$total <- ave(data$count, data$cardio, FUN = sum)
  data$percentage <- (data$count / data$total) * 100
  ggplot(data, aes(x = as.factor(cardio), y = count, fill = as.factor(group))) +
    geom_bar(stat = "identity", position = "stack") +
    geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5), size = 3) +
    labs(title = title, x = x_label, y = y_label, fill = fill_label) +
    scale_fill_manual(values = fill_colors) +
    theme_minimal()
}

pChol <- stacked_bplot(cardio, "cardio", "cholesterol", c("royalblue", "gold", "lightcoral"),
  "Population by Cholesterol \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Cholesterol"
)

pGluc <- stacked_bplot(cardio, "cardio", "gluc", c("royalblue", "gold", "lightcoral"),
  "Population by Glucose \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Glucose"
)

pSmoke <- stacked_bplot(cardio, "cardio", "smoke", c("royalblue", "lightcoral"),
  "Population by Smoking Status \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Smoking"
)

pActive <- stacked_bplot(cardio, "cardio", "active", c("lightcoral", "royalblue"),
  "Population by Physical Activity \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Active"
)

pAlco <- stacked_bplot(cardio, "cardio", "alco", c("royalblue", "lightcoral"),
  "Population by Alcohol Consumption \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Alcohol"
)

pGender <- stacked_bplot(cardio, "cardio", "gender", c("lightcoral", "royalblue"),
  "Population by Gender \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Gender"
)

pAgeCat <- stacked_bplot(cardio, "cardio", "age_cat", c("royalblue", "lightcoral"),
  "Population by Age Category \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Age Category"
)

chol_gluc = grid.arrange(pChol, pGluc, ncol = 2)
smoke_active = grid.arrange(pSmoke, pActive, ncol = 2)
alcohol_gender = grid.arrange(pAlco, pGender, ncol = 2)
age_cat_barplot = grid.arrange(pAgeCat, ncol = 2)
```

```{r SAVING BARPLOTS}
# saving barplots
cg <- arrangeGrob(pChol, pGluc, ncol = 2)
sa <- arrangeGrob(pSmoke, pActive, ncol = 2)
ag <- arrangeGrob(pAlco, pGender, ncol = 2)
acat <- arrangeGrob(pAgeCat, ncol = 1)
ggsave("images/chol_gluc.png", cg, width = 8, height = 8)
ggsave("images/smoke_active.png", sa, width = 8, height = 8)
ggsave("images/alcohol_gender.png", ag, width = 8, height = 8)
ggsave("images/agecat.png", acat, width = 8, height = 8)
```

Within people having heart a disease, high cholesterol, glucose and older age people are significantly overrepresented, and to a smaller extent are also inactive people. Surprisingly, the percentage of smokers and alcohol consumers is slightly higher for the healthy group, whereas gender doesn't seem to have significant differences.



### Correlation analysis

The following heatmap confirms some (perhaps rather intuitive) relationships:

- Diastolic (`ap_lo`) and systolic (`ap_hi`) blood pressure are heavily correlated
- Both `weight` and `age` are positively correlated with higher diastolic and systolic blood pressure, the former slightly more than the latter.\

```{r}
cor_cardio <- round(cor(as.data.frame(cardio_cl)), 2)

melt_cardio <- melt(cor_cardio, na.rm = TRUE)  
labels <- c(
  "age","gender*", "height", "weight", "ap_hi", "ap_lo", "cholesterol*", "gluc*", "smoke*", "alco*", "active*", "cardio*")  
corr_heatmap<- ggplot(data = melt_cardio, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() +
  geom_text(aes(label = value), size = 4) + 
  scale_fill_gradient2(low = "blue", high = "red", limit = c(-1, 1), name = "Correlation") +
  scale_x_discrete(labels = labels) +  
  scale_y_discrete(labels = labels) + 
  theme_minimal() +  labs(caption = "Variables marked with * are categorical.") +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    axis.text.y = element_text(size = 10)  
  )

corr_heatmap
ggsave("images/corrheatmap.png", corr_heatmap, width = 8, height = 8)

```


We use the Chi-squared test to see to which degree the variables are correlated with `cardio`:

```{r}
categorical_vars <- names(categorical)[names(categorical) != "cardio"]
test_results <- list()

for (var in categorical_vars) {
  test <- chisq.test(table(cardio$cardio, cardio[[var]]))
  test_results[[var]] <- test
}

for (var in categorical_vars) {
  cat("Chi-Squared Test for", var, ":\n")
  print(test_results[[var]])
  cat("\n")
}

significant_vars <- names(test_results)[sapply(test_results, function(x) x$p.value < 0.05)]
cat("Variables with significant association (p < 0.05):\n")
print(significant_vars)
```

We see that all of the available categorical variables, besides `gender`, seem to be very relevant in determining the insurgence of heart diseases.

Let's now test the mean differences between groups for diastolic and systolic blood pressure. We perform t-tests for binary variables, and ANOVA tests for groups of more than two.

```{r}
continuous_vars = c("ap_hi", "ap_lo", "weight", "height", "age")

test_mean_diff = function(continuous_vars, categorical_vars){
  mean_diff_results <- data.frame(
    Variable = character(),
    Group = character(),
    Test = character(),
    P_Value = numeric(),
    stringsAsFactors = FALSE
)
  for (cont_var in continuous_vars) {
  for (cat_var in categorical_vars) {
    num_levels = length(unique(cardio[, cat_var]))
    if (num_levels == 2) {
      # Perform t-test
      t_test = t.test(cardio[, cont_var] ~ cardio[, cat_var])
      p_value = t_test$p.value
      test_type = "t-test"
    } else {
      # Perform ANOVA
      anova_test = aov(cardio[, cont_var] ~ cardio[, cat_var])
      p_value = summary(anova_test)[[1]][["Pr(>F)"]][1]
      p_value = 0.0001
      test_type = "ANOVA"
    }
    # Append results
    results = data.frame(
                                Variable = cont_var, 
                                Group = cat_var, 
                                Test = test_type,
                                P_Value = p_value
                              )
    mean_diff_results = bind_rows(mean_diff_results, results)
  }
  }
return(mean_diff_results)
  }

# View results
result = test_mean_diff(continuous_vars, categorical_vars)
print(result)
```

Once again, there definitely are some significant differences between the groups considered.

```{r}
custom_colors <- c("darkviolet","orange", "#228b22")

create_plot <- function(color_var) {
  ggplot(cardio, aes(cardio, ap_hi)) +
    geom_boxplot(aes(color = !!sym(color_var))) +
    scale_color_manual(values = custom_colors) + theme_minimal()
}

p1 <- create_plot("cholesterol")
p2 <- create_plot("age_cat")
p3 <- create_plot("smoke")
p4 <- create_plot("active")
p5 <- create_plot("alco")
p6 <- create_plot("gluc")

```


```{r}
grid.arrange(p1, p2, ncol = 2)
grid.arrange(p3, p4, ncol = 2)
grid.arrange(p5, p6, ncol = 2)
```

```{r SAVE SCATTERPLOTS}
first4cat = arrangeGrob(p1, p2, p3, p4, ncol=2)
second2cat = arrangeGrob(p5, p6, ncol=2)

ggsave("images/APHI_chol_age_smoke_active.png", first4cat, width = 8, height = 8)
ggsave("images/APHIalco_gluc.png", second2cat, width = 8, height = 8)

```

While no group shows a clean division with respect to `cardio`, values of both `gluc` and `cholesterol` *above normal* or *well above normal* seem to be particularly frequent along with the occurrence of heart diseases.


## Cluster analysis

We proceed by performing cluster analysis. Due to the nature of data, and various experiments with the results, we considered two different approaches:

- Considering all the variables, but scaling just the numerical ones.
- Scaling and considering just numerical variables.

### K-means clustering (all variables)

```{r}
# Filter the dataset to exclude these columns and scale the remaining numeric columns
cardio_scaled_onlynums = cardio_cl
cardio_scaled_onlynums[, c(continuous_vars)] = scale(cardio_scaled_onlynums[, c(continuous_vars)])

# View the first few rows of the dataset
head(cardio_scaled_onlynums)
```

```{r}
# Compute the distance matrix for the scaled data
sample_cardio_onlynums = sample(as.matrix(cardio_scaled_onlynums), 10000)
dist_matrix <- dist(sample_cardio_onlynums)
```


```{r}
# Perform K-means clustering
set.seed(123)  
silhouette_scores_onlynum = c()

# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)
# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_clusters in 2:10) {
   # Perform k-means clustering
   kmeans_result <- kmeans(sample_cardio_onlynums, centers = n_clusters)

   # Extract cluster labels from the k-means result
   cluster_labels <- kmeans_result$cluster

   # Calculate silhouette values
   sil <- silhouette(cluster_labels, dist_matrix)

   # Store the average silhouette score for the current number of clusters
   silhouette_scores_onlynum[n_clusters] <- mean(sil[, 3])
 }

max_silhouette_kmeans_onlynum = which.max(silhouette_scores_onlynum)

best_kmeans_result_onlynum <- kmeans(cardio_cl, centers = max_silhouette_kmeans_onlynum)
# Add the cluster assignment to the original data
cardio$Cluster_ON <- as.factor(best_kmeans_result_onlynum$cluster)
table(cardio$Cluster_ON)
```

```{r}
# Visualize the clusters across different dimensions
cl_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster_ON)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
cl_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster_ON)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(cl_p1, cl_p2)
```

```{r}
cl_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster_ON)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(cl_p3, cardio_pl)
```

There is a split, but a lot of clusters overlap for central values.
```{r}
cluster_summary_ON <- cardio %>%
  group_by(Cluster_ON) %>%
  summarize(
    Count = n(),
    disease_incidence = sum(cardio == 1)/n(),
    active_count = sum(active == "Yes")/n(),
    high_chol = sum(cholesterol %in% c("above normal", "well above normal"))/n(),
    gluc = sum(gluc %in% c("above normal", "well above normal"))/n(),
    ap_hi = mean(ap_hi),
    ap_lo = mean(ap_lo),
    Female = sum(gender == "F") / n(), 
    Male = sum(gender == "M") / n() 
    # Add more summary statistics as needed
  )
cluster_summary_ON
```

### K-means clustering (numerical variables)
```{r}
cardio_justnum <- scale(cardio_cl[, continuous_vars])

# View the first few rows of the dataset
head(cardio_justnum)
```

```{r}
# Compute the distance matrix for the scaled data
sample_justnum = sample(cardio_justnum, 10000)
dist_matrix_justnum <- dist(sample_justnum)
```

```{r}
# Perform K-means clustering
set.seed(123)  
silhouette_scores_justnum = c()

# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)
# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_clusters in 2:10) {
   # Perform k-means clustering
   kmeans_result <- kmeans(sample_justnum, centers = n_clusters)

   # Extract cluster labels from the k-means result
   cluster_labels <- kmeans_result$cluster

   # Calculate silhouette values
   sil <- silhouette(cluster_labels, dist_matrix)

   # Store the average silhouette score for the current number of clusters
   silhouette_scores_justnum[n_clusters] <- mean(sil[, 3])
 }

max_silhouette_kmeans_justnum = which.max(silhouette_scores_justnum)

best_kmeans_result_justnum <- kmeans(cardio_justnum, centers = max_silhouette_kmeans_justnum)
# Add the cluster assignment to the original data
cardio$Cluster <- as.factor(best_kmeans_result_justnum$cluster)
table(cardio$Cluster)
```

```{r}
# Visualize the clusters across different dimensions
cl_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
cl_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(cl_p1, cl_p2)
```

```{r}
cl_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster)) +
  geom_point() +
  scale_color_manual(values = c("1" = "blue", "2" = "red")) + # Invert cluster colors
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)", color = "Cluster")
cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  scale_color_manual(values = c("0" = "red", "1" = "blue")) + # Invert cluster colors
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(cl_p3, cardio_pl)
```
```{r}
ggsave("images/2kmodes_justnum_good.png", width=8, height=6)
```

```{r}
info_cluster1 = cardio %>% filter(Cluster == 1)
summary(info_cluster1)
```
```{r}
info_cluster2 = cardio %>% filter(Cluster == 2)
summary(info_cluster2)
```
```{r}
# Group the data by Cluster and calculate summary statistics
cluster_summary <- cardio %>%
  group_by(Cluster) %>%
  summarize(
    Count = n(),
    disease_incidence = sum(cardio == 1)/n(),
    active_count = sum(active == "Yes")/n(),
    high_chol = sum(cholesterol %in% c("above normal", "well above normal"))/n(),
    gluc = sum(gluc %in% c("above normal", "well above normal"))/n(),
    Female = sum(gender == "F") / n(), 
    Male = sum(gender == "M") / n(),
    ap_hi = mean(ap_hi),
    ap_lo = mean(ap_lo),
    # Add more summary statistics as needed
  )

# Print the summary table
print(cluster_summary)
```

Now one can clearly see how the clusters different in terms of health: there is a very high incidence of cardiovascular disease in the first cluster, which has also predictably higher levels of cholesterol and glucose present in their blood. Gender percentages are almost equal, with slightly more females being in the "healthier" cluster. Pressures also differ across the two, with $\approx 25$ points difference for systolic and $\approx 14$ for diastolic blood pressure. 

```{r remove cluster labels}
cardio$Cluster <- NULL
cardio$Cluster_ON <- NULL
cardio$age_cat <- NULL
```



## Random Forest

### Classification trees

In order to correctly fit the random forest model, let's analyze first some simpler classifiers: classification trees. 

For this models, as well as for the following ones, dataset will be split in two parts: 80% for training the model and 20% for testing. 

```{r classification trees (high cp) 1}
# Split data (train/test)
set.seed(123)
train_index <- sample(1:nrow(cardio), 0.8 * nrow(cardio))  
train_data <- cardio[train_index, ]
test_data <- cardio[-train_index, ]
```

Now, it is time to fit the model. The function `rpart`, expects some user-defined parameters (the ones passed to the function below) but can also accept multiple optional parameters like `rpart.control`. This one controls some of the characteristic of the tree:

- `minsplit`: Minimum observations to attempt a split.
- `cp`: Complexity parameter for pruning.
- `maxdepth`: Maximum depth of the tree
- ...

```{r classification trees (high cp) 2}
# Build and plot the classification tree
tree <- rpart(cardio ~ ., data = train_data, method = "class")
tree$control

rpart.plot(tree)

# Save image
png("images/high_cp_tree_plot.png", width = 800, height = 600)
rpart.plot(tree)
dev.off()
```

At first sight, we can observe that the classification tree completely relies on the feature `ap_hi` for the only split. Let's check the importance of the different features in the model:

```{r classification trees (high cp) 3}
# Check feature importance
tree$variable.importance
```

It seem that `ap_hi`, together with `ap_lo`, out stand over the others features. Even though both of them are significant, they are highly correlated, so taking them into account at the same time would introduce redundancy in the tree.

In order to better understand the performance of this model, we can check its accuracy and the area under the ROC curve after trying to classify the testing data.

```{r classification trees (high cp) 4}
# Make predictions
predictions <- predict(tree, test_data, type = "prob")

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions[, 2] > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)
confusion_matrix

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# ROC curve
roc_curve <- roc(test_data$cardio, predictions[, 2])
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))

# Save image
png("images/high_cp_roc_curve.png", width = 800, height = 600)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
```

Could we improve the performance of the model with more splits? In order to force the tree to use less relevant features, we will readjust the complexity parameter (`cp`).

```{r classification trees (low cp) 1}
# New classification tree
tree <- rpart(cardio ~ ., data = train_data, method = "class",
              control = rpart.control(cp = 0.001))
rpart.plot(tree)

# Save image
png("images/low_cp_tree_plot.png", width = 800, height = 600)
rpart.plot(tree)
dev.off()
```

This time a much complex tree is created, taking into account all the features. All variables have now some importance in the model (even though the differences among them can be huge).

```{r classification trees (low cp) 2}
# Check feature importance
tree$variable.importance
```

Let's check the performance:

```{r classification trees (low cp) 3}
# Make predictions
predictions <- predict(tree, test_data, type = "prob")

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions[, 2] > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)
confusion_matrix

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# ROC curve
roc_curve <- roc(test_data$cardio, predictions[, 2])
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))

# Save image
png("images/low_cp_roc_curve.png", width = 800, height = 600)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
```

After evaluating both models, we get a sightly better accuracy from a significantly more complex tree. Moreover, the area under the ROC curve also increases. From this comparison, we can conclude that although the blood pressure is one the best indicators (in this dataset) of cardiovascular diseases, observing other variables like cholesterol, weight, age or glucose also helps to identify them.


### Model

Following classification trees, a natural approximation is fitting a random forest model. 

Since the trees that make up the model are completely independent and in order to speed up the training, the problem may be parallelized. Let's set it up:

```{r parellization}
# Detect the number of cores available on your machine and register them
cores <- parallel::detectCores()
cl <- makeCluster(cores)  # Create a cluster with the detected cores
registerDoParallel(cl)    # Register the parallel backend
```

For this model, instead of splitting the data in two as we previously did, we will apply a k-fold-cross-validation technique. As this is going to be one of the final classifiers, correctly evaluating its performance and fully using the dataset for both training and validation is of vital importance. 

Many parameters can be configured here. Some of them are:

- `trControl(folds)`: The number of folds for the cross validation.
- `tuneGrid(mtry)`: The number of features to use in each decision tree.
- `tuneGrid(splitrule)`: Rule for splitting the tree
- `tuneGrid(min.node.size)`: Minimum number of observations per leaf node.
- ...

After testing and analyzing all type of combination, the one which provided the best area under the ROC curve was the following one:

```{r random forest (training) 1}
# Cross-validation setup
train_control <- trainControl(
  method = "cv",          
  number = 5,           # Folds
  classProbs = TRUE,     # Enable probabilities
  summaryFunction = twoClassSummary
)

tune <- expand.grid(
    mtry = c(2),
    splitrule = c("gini"),
    min.node.size = c(1)
)
```

Finally, let's compute the random forest using our settings.

```{r random forest (training) 2}
# Factorize the variable if it is not a factor
train_data$cardio <- factor(train_data$cardio, levels = c(0, 1), labels = c("No", "Yes"))

rf_model <- train(
  cardio ~ ., 
  data = train_data,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune,
  metric = "ROC"
)

# Free resources
stopCluster(cl)
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
unregister_dopar()

# Final model
print(rf_model)
```


### Performance

Regarding the performance of the final model:

```{r random forest (performance) 3}
# General results
rf_model$results

# Get predicted probabilities
predictions <- predict(rf_model, newdata = test_data, type = "prob")
predicted_probs <- predictions[, "Yes"]
actual_labels <- test_data$cardio 

# Manually compute the area under ROC curve
roc_curve_rf <- roc(as.numeric(actual_labels), predicted_probs)

# ROC curve
plot(roc_curve_rf, main = "ROC Curve", col = "blue", lwd = 2)
rf_auc <- auc(roc_curve_rf)

# Save image
png("images/rf_roc_curve.png", width = 800, height = 600)
plot(roc_curve_rf, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
```

```{r rf_model_confusion_matrix}
## Confusion Matrix for Random Forest
predicted_class <- ifelse(predicted_probs >= 0.5, "Yes", "No")
levels(actual_labels) <- c("No", "Yes")
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))

# Convert the confusion matrix into a table
cm_table <- as.data.frame(cm$table)
cm_table$Reference <- factor(cm_table$Reference, levels = rev(levels(as.factor(actual_labels))))
colnames(cm_table) <- c("Prediction", "Reference", "Count")

# Plot the confusion matrix
rf_confusion_matrix <- ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Count)) +
  geom_tile() +
  geom_text(aes(label = Count), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Confusion Matrix: Random Forest (t = 0.50)", x = "Actual", y = "Predicted")
ggsave("images/rf_confusion_matrix_50.png", rf_confusion_matrix, width = 8, height = 8)
rf_confusion_matrix
```

```{r rf_model_other_metrics}
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)

# Print computed metrics
print(metrics_df)
```

As we could imagine, this enhanced version of classification trees gets a better performance in the classification task. However, we must take into account the high computational complexity of this new model compared with the previous ones.

If we observe the confusion matrix above, we easily realize that the model tends to underpredict the disease. Given the healthcare context in which we are, a lower threshold for the matrix should also be taken into account.


```{r rf_model_confusion_matrix_OTHER_THRESHOLD}
## Confusion Matrix for Random Forest
predicted_class <- ifelse(predicted_probs >= 0.45, "Yes", "No")
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))

# Convert the confusion matrix into a table
cm_table <- as.data.frame(cm$table)
cm_table$Reference <- factor(cm_table$Reference, levels = rev(levels(as.factor(actual_labels))))
colnames(cm_table) <- c("Prediction", "Reference", "Count")

# Plot the confusion matrix
rf_confusion_matrix <- ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Count)) +
  geom_tile() +
  geom_text(aes(label = Count), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Confusion Matrix: Random Forest (t = 0.45)", x = "Actual", y = "Predicted")
ggsave("images/rf_confusion_matrix_45.png", rf_confusion_matrix, width = 8, height = 8)
rf_confusion_matrix
```

```{r rf_model_other_metrics_OTHER_THRESHOLD}
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
rf_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)

# Print computed metrics
print(rf_metrics_df)
```

With this new value, not only the number of mispredictions was reduced, but also the error tendency was flipped. The new matrix shows how the model now overpredicts the disease, which is a safer option.



## Logistic Regression

### Basic Model

We fit some logistic regression models. These models are mainly used when the target variable is dichotomous, as in this case, where only two classes can be predicted. They to map the output of the model into a meaningful range for probabilities([0, 1]).

Let's start building a general model, without interaction between variables. Again, a 5-fold cross-validation will be applied.

```{r basic_logistic_regression}
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) 
levels(cardio$cardio) <- c("No", "Yes")
levels(train_data$cardio) <- c("No", "Yes")
levels(test_data$cardio) <- c("No", "Yes")


# Cross-validation
log_model <- train(
  cardio ~ .,                    # Formula
  data = train_data,             # Dataset
  method = "glm",                # Correct method for logistic regression
  family = binomial(link = "logit"),  # Logistic regression
  trControl = ctrl,              # Cross-validation settings
  metric = "ROC"            # Appropriate metric for classification
)

# Print the model
print(log_model)
```

At first sight, the results obtained are similar to the ones from the previous model. 

Let's plot the decision boundary with respect `ap_hi` and `ap_lo` to better see the relation between both variables:
```{r lr_decision_boundaries}
grid <- expand.grid(
  ap_hi = seq(min(test_data$ap_hi), max(test_data$ap_hi), length.out = 200),
  ap_lo = seq(min(test_data$ap_lo), max(test_data$ap_lo), length.out = 200)
)

# For other predictors, calculate medians for numerical variables and most frequent levels for factors
other_predictors <- test_data %>%
  select(-c(ap_hi, ap_lo, cardio)) %>%
  summarise(across(where(is.numeric), median), across(where(is.factor), ~ names(sort(table(.), decreasing = TRUE)[1])))

# Repeat median/frequent level values for all rows of the grid
grid <- cbind(grid, other_predictors[rep(1, nrow(grid)), ])

# Convert factor levels in the grid to match the cardio dataset's levels
grid <- grid %>%
  mutate(across(where(is.factor), ~ factor(., levels = levels(cardio[[deparse(substitute(.))]]))))

# Predict probabilities for the grid
probabilities <- predict(log_model, newdata = grid, type = "prob")
grid$predicted_probs <- probabilities[, 2]  # Extract probabilities for the positive class

# Plot decision boundary
decision_boundary <- ggplot() +
  # Decision boundary contour
  geom_contour(data = grid, aes(x = ap_hi, y = ap_lo, z = predicted_probs), breaks = 0.5, color = "black", linetype = "solid", linewidth = 1) +
  
  # Filled contour plot
  geom_tile(data = grid, aes(x = ap_hi, y = ap_lo, fill = predicted_probs), alpha = 0.8) +
  scale_fill_gradient(low = "#6699FF", high = "#FFCCCC", name = "P(cardio=Yes)") +
  
  # Original data points
  geom_point(data = test_data, aes(x = ap_hi, y = ap_lo, color = as.factor(cardio)), size = 2.5) +
  scale_color_manual(values = c("No" = "#33FF57", "Yes" = "#FF5733"), name = "Cardio") +
  
  # Titles and themes
  labs(
    title = "Logistic Regression Decision Boundary",
    x = "Systolic Blood Pressure (ap_hi)", y = "Diastolic Blood Pressure (ap_lo)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "right"
  )

# Save the plot
ggsave("images/decision_boundary_cardio.png", decision_boundary, width = 8, height = 8)

# Display the plot
decision_boundary
```

Even though some red and green points are misclassified, the great majority are correctly predicted. We can see how both pressures play a significant role in determining the probability of having a cardio disease, providing complementary information for classification.

### Performance

Let's compute some performance metrics:
```{r log_model_performance}
# Get predicted probabilities
predictions <- predict(log_model, newdata = test_data, type = "prob")
predicted_probs <- predictions[, "Yes"] 
actual_labels <- test_data$cardio 

# Manually compute the area under ROC curve
roc_curve_lr <- roc(actual_labels, predicted_probs)

# ROC curve
plot(roc_curve_lr, main = "ROC Curve", col = "blue", lwd = 2)
lr_auc <- auc(roc_curve_lr)

# Save image
png("images/lr_roc_curve.png", width = 800, height = 600)
plot(roc_curve_lr, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
```

```{r log_model_confusion_matrix}
## Confusion Matrix for Logistic Regression
predicted_class <- ifelse(predicted_probs >= 0.5, "Yes", "No")
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))

# Convert the confusion matrix into a table
cm_table <- as.data.frame(cm$table)
cm_table$Reference <- factor(cm_table$Reference, levels = rev(levels(as.factor(actual_labels))))
colnames(cm_table) <- c("Prediction", "Reference", "Count")
cm
# Plot the confusion matrix
lr_confusion_matrix <- ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Count)) +
  geom_tile() +
  geom_text(aes(label = Count), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Confusion Matrix: Logistic Regression (t = 0.50)", x = "Actual", y = "Predicted")
ggsave("images/lr_confusion_matrix_50.png", lr_confusion_matrix, width = 8, height = 8)
lr_confusion_matrix
```

```{r lg_model_other_metrics}
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)

# Print computed metrics 
print(metrics_df)
ratio <- metrics_df$Value[7]/metrics_df$Value[6]
```

The False Positive Rate (FPR) is higher than the False Negative Rate (FPR) by a factor of $1.25$. Considering the medical context, we again try to lower the threshold to see if we can balance the ratio without losing too much in terms of accuracy. 


```{r log_model_confusion_matrix_OTHER_TRESHOLD}
## Confusion Matrix for Logistic Regression
predicted_class <- ifelse(predicted_probs >= 0.45, "Yes", "No")
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))

# Convert the confusion matrix into a table
cm_table <- as.data.frame(cm$table)
cm_table$Reference <- factor(cm_table$Reference, levels = rev(levels(as.factor(actual_labels))))
colnames(cm_table) <- c("Prediction", "Reference", "Count")

# Plot the confusion matrix
lr_confusion_matrix <- ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Count)) +
  geom_tile() +
  geom_text(aes(label = Count), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Confusion Matrix: Logistic Regression (t = 0.45)", x = "Actual", y = "Predicted")
ggsave("images/lr_confusion_matrix_45.png", lr_confusion_matrix, width = 8, height = 8)
lr_confusion_matrix
```

```{r lg_model_other_metrics_OTHER_TRESHOLD}
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
lr_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)

# Print computed metrics
print(lr_metrics_df)
```
Considering that in a medical context it is often preferred to overpredict than to underpredict values, this lower threshold is preferred: while the false positive rate is increased, we are overall able to correctly classify more patients.

Understanding the contribution of each feature to the model could also help us explain the model in more depth:
```{r lr-feature-importance}
# Feature Importance
summary(log_model)
importance_lr <- as.data.frame(summary(log_model)$coefficients)
importance_lr$Feature <- rownames(importance_lr)

# Remove intercept for better visualization
importance_lr <- importance_lr[importance_lr$Feature != "(Intercept)", ]

# Rename columns for clarity
colnames(importance_lr) <- c("Estimate", "StdError", "ZValue", "PValue", "Feature")

# Plot feature importance
feature_importance <- ggplot(importance_lr, aes(x=reorder(Feature, Estimate), y=Estimate)) +
  geom_bar(stat='identity', fill='steelblue') +
  coord_flip() +
  labs(title="Coefficient Contribution - Logistic Regression",
       x="Feature", y="Value)") +
  theme_minimal()
ggsave("images/feature_importance.png", feature_importance, width = 8, height = 8)
feature_importance
```

It is important to note that the plot compares categorical and numerical values. Scales are completely different (an increment of one unit in `cholesterol`, for example, is much more significant than the same increment in `ap_hi`). However, we can distinguish in an easy way which variables contribute positively or negatively to the prediction.


### Interaction between variables

#### Smoke & Alcohol Model

What if we introduce interaction between some of the variables? As we saw before, some pairs of features are highly correlated, e.g., `smoke` and `alco` or `gluc` and `cholesterol`. A reasonable approach could be building some models based on them.
```{r}
smoke_check = glm(cardio ~ smoke + alco, data = train_data, family = binomial(link = "logit") )
summary(smoke_check)
```

Even if taken as the single predictor, they still counterintuitively contribute negatively to `cardio`. However the significance is lower than the one in the complete model.

In the data exploration part, we saw that there were, numerically, minimal differences between groups with regard to these variables. Furthermore both of these variables are *subjective*, meaning that the answer was at the respondent's discretion. This means one could straight up lie, or even, having quit recently, answer *no* even though the effects of alcohol consumption or smoking were already present. That is a fact to be taken into consideration.


#### Glucose & Cholesterol Model

Let's try with the second pair of variables.
```{r}
gluc_check = glm(cardio ~ gluc + cholesterol, data = train_data, family = binomial(link = "logit"))
summary(gluc_check)
```

High levels of cholesterol are positively correlated with high glucose, as expected.
```{r}
interaction_gluc_cardio = glm(cardio ~ gluc*cholesterol, data = train_data, family = binomial(link = "logit"))
summary(interaction_gluc_cardio)
```

The weird effect is present just on levels of `gluc` *above normal* when they interact with high cholesterol. Taken individually, when they are significant, they contribute positively to the insurgence of heart diseases, as expected.  


#### Pulse Pressure Model

It would be interesting to work with *pulse pressure* too. It is an important mortality predictor and, according to some researchers, it appears to be a stronger independent predictor of mortality than other parameters such as systolic or diastolic pressures by themselves [@malone2010pulse]. 

Pulse pressure is a function consisting on the difference of systolic and diastolic pressures: $ap\_pu = ap\_hi - ap\_lo$.

```{r}
cardio_cl$ap_pu = cardio$ap_hi - cardio$ap_lo
log_model_ap_pu <- train(
  cardio ~ . -ap_hi-ap_lo,       # Formula
  data = train_data,             # Dataset
  method = "glm",                # Correct method for logistic regression
  family = binomial(link = "logit"),  # Logistic regression
  trControl = ctrl,              # Cross-validation settings
  metric = "ROC"            # Appropriate metric for classification
)

print(log_model_ap_pu)
summary(log_model_ap_pu)
```

Nevertheless, results seem to be similar, if not worse, compared with previous models.


### Polynomial Regressions

#### Residuals

Let's check the residual plots, using binned residuals:
```{r original_residuals 1}
fit <- glm(cardio ~ ., data = cardio, family = binomial(link = "logit"))
pred <- predict(fit, type = "response")
res <- residuals(fit, type = "response")

cardio$cardio <- factor(cardio$cardio, levels = c("No", "Yes"), labels = c(0, 1))

binnedplot(pred,res, nclass = 1000)
```

Let's see if a polynomial regression can improve the model's fit.

```{r}
fit_poly3 <- glm(cardio ~ .+poly(ap_hi, 3), data = cardio, family = binomial(link = "logit"))

pred <- predict(fit_poly3, type = "response")
res <- residuals(fit_poly3, type = "response")
summary(fit_poly3)
```

```{r}
binnedplot(pred,res, nclass = 1000)
```

```{r}
fit_poly4 <- glm(cardio ~ .+poly(ap_hi, 4), data = cardio, family = binomial(link = "logit"))

pred <- predict(fit_poly4, type = "response")
res <- residuals(fit_poly4, type = "response")
summary(fit_poly4)
```

```{r}
binnedplot(pred,res, nclass = 1000)
```

Including a polynomial term for `ap_hi` in our model, the strongly non-linear pattern seen in the residuals of the "vanilla" model seems to become sparser, even though there is still some unexplained variance in the plot. 
All degrees of the polynomial for systolic blood pressure are significant, improving the model's fit. 


#### Polynomial of degree 3
```{r}
# Cross-validation
poly3 <- train(
  cardio ~ . + poly(ap_hi, 3),   # Formula
  data = train_data,             # Dataset
  method = "glm",                # Correct method for logistic regression
  family = binomial(link = "logit"),  # Logistic regression
  trControl = ctrl,              # Cross-validation settings
  metric = "ROC"            # Appropriate metric for classification
)

# General results
poly3$results

# Get predicted probabilities
predictions <- predict(poly3, newdata = test_data, type = "prob")
predicted_class <- ifelse(predicted_probs >= 0.45, "Yes", "No")
predicted_probs <- predictions[, "Yes"] 
actual_labels <- test_data$cardio 

# Manually compute the area under ROC curve
roc_curve_poly3 <- roc(actual_labels, predicted_probs)
auc_poly3 <- auc(roc_curve_poly3)

# Create confusion matrix
cm <- confusionMatrix(
  factor(predicted_class),
  factor(actual_labels),
  positive = "Yes"
)

## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
poly3_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)

# ROC curve
plot(roc_curve_poly3, main = "ROC Curve for poly3", col = "blue", lwd = 2)
```
```{r}
roc_curve_poly3$auc
```

#### Polynomial of degree 4
```{r}
# Cross-validation
poly4 <- train(
  cardio ~ . + poly(ap_hi, 4),   # Formula
  data = train_data,             # Dataset
  method = "glm",                # Correct method for logistic regression
  family = binomial(link = "logit"),  # Logistic regression
  trControl = ctrl,              # Cross-validation settings
  metric = "ROC"            # Appropriate metric for classification
)

# General results
poly4$results

# Get predicted probabilities
predictions <- predict(poly4, newdata = test_data, type = "prob")
predicted_class <- ifelse(predicted_probs >= 0.45, "Yes", "No")
predicted_probs <- predictions[, "Yes"] 
actual_labels <- test_data$cardio 

# Manually compute the area under ROC curve
roc_curve_poly4 <- roc(actual_labels, predicted_probs)
auc_poly4 <- auc(roc_curve_poly4)

# Create confusion matrix
cm <- confusionMatrix(
  factor(predicted_class),
  factor(actual_labels),
  positive = "Yes"
)

## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
poly4_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)
# ROC curve
plot(roc_curve_poly4, main = "ROC Curve for poly4", col = "blue", lwd = 2)
```
```{r}
roc_curve_poly4$auc
```


#### LASSO

It's possible to see that by considering the polynomials of the systolic blood pressure, the residuals follow less of a pattern. Further factors (like higher polynomials and multiple interactions) were also considered but they did not contribute to improving the model. 

We still get strange values for some coefficients (e.g. `gluc`).

We apply LASSO regression to see what coefficients are actually important.
```{r}
set.seed(23)
# Model matrix creation
train_data_dummies <- model.matrix(cardio ~ ., data = train_data)[, -1]  # Exclude intercept column
test_data_dummies <- model.matrix(cardio ~ ., data = test_data)[, -1]    # Exclude intercept column

X_train <- scale(train_data_dummies)
Y_train <- as.numeric(train_data$cardio) - 1  # Assuming 'cardio' is a factor variable (0/1)
X_test <- scale(test_data_dummies)
Y_test <- as.numeric(test_data$cardio) - 1  # Assuming 'cardio' is a factor variable (0/1)

# Fit the model using cv.glmnet
cvfit <- cv.glmnet(X_train, Y_train, family = "binomial", type.measure = "class", alpha = 1)
plot(cvfit)
```


```{r}
print(cvfit)
```

By using a $\lambda$ which produces a misclassification error within 1 standard error from the minimum's, we obtain similar results, while also simplifying the model: those problematic coefficients from before are shrunk to 0, underlying their secondary importance in determining the presence of heart disease. This is indeed consistent with what we had supposed in our initial exploration of the data. 

```{r}
# Get optimal lambda values
lambda_min <- cvfit$lambda.min
lambda_1se <- cvfit$lambda.1se
cat("Lambda.min:", lambda_min, "\n")
cat("Lambda.1se:", lambda_1se, "\n")
```


```{r}
# Coefficients
cat("Coefficients at lambda.min:\n")
print(coef(cvfit, s = "lambda.min"))
```


```{r}
cat("Coefficients at lambda.1se:\n")
print(coef(cvfit, s = "lambda.1se"))
```


```{r}
# Predictions on test data
predictions <- predict(cvfit, newx = X_test, s = "lambda.min", type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
```


```{r}
# Training predictions (lambda.min)
train_predictions_min <- predict(cvfit, newx = X_train, s = "lambda.min", type = "response")
train_pred_classes_min <- ifelse(train_predictions_min > 0.5, 1, 0)
train_accuracy_min <- mean(train_pred_classes_min == Y_train)
train_roc_min <- roc(Y_train, as.vector(train_predictions_min))
train_conf_matrix_min <- table(Predicted = train_pred_classes_min, Actual = Y_train)

# Test predictions (lambda.min)
test_predictions_min <- predict(cvfit, newx = X_test, s = "lambda.min", type = "response")
test_pred_classes_min <- ifelse(test_predictions_min > 0.5, 1, 0)
test_accuracy_min <- mean(test_pred_classes_min == as.numeric(test_data$cardio) - 1)
test_roc_min <- roc(as.numeric(test_data$cardio) - 1, as.vector(test_predictions_min))
test_conf_matrix_min <- table(Predicted = test_pred_classes_min, Actual = as.numeric(test_data$cardio) - 1)

# Training predictions (lambda.1se)
train_predictions_1se <- predict(cvfit, newx = X_train, s = "lambda.1se", type = "response")
train_pred_classes_1se <- ifelse(train_predictions_1se > 0.5, 1, 0)
train_accuracy_1se <- mean(train_pred_classes_1se == Y_train)
train_roc_1se <- roc(Y_train, as.vector(train_predictions_1se))
train_conf_matrix_1se <- table(Predicted = train_pred_classes_1se, Actual = Y_train)


# Test predictions (lambda.1se)
test_predictions_1se <- predict(cvfit, newx = X_test, s = "lambda.1se", type = "response")
test_pred_classes_1se <- ifelse(test_predictions_1se > 0.5, 1, 0)
test_accuracy_1se <- mean(test_pred_classes_1se == as.numeric(test_data$cardio) - 1)
test_roc_1se <- roc(as.numeric(test_data$cardio) - 1, as.vector(test_predictions_1se))
test_conf_matrix_1se <- table(Predicted = test_pred_classes_1se, Actual = as.numeric(test_data$cardio) - 1)


# Calculate MSE for training and test sets
train_mse_min <- mean((train_predictions_min - Y_train)^2)
train_mse_1se <- mean((train_predictions_1se - Y_train)^2)
test_mse_min <- mean((test_predictions_min - (as.numeric(test_data$cardio) - 1))^2)
test_mse_1se <- mean((test_predictions_1se - (as.numeric(test_data$cardio) - 1))^2)


# Create a summary table
results <- data.frame(
  Metric = c(
    "Training Accuracy", "Training AUC", "Training MSE",
    "Test Accuracy", "Test AUC", "Test MSE"
  ),
  Lambda_Min = c(
    train_accuracy_min, auc(train_roc_min), train_mse_min,
    test_accuracy_min, auc(test_roc_min), test_mse_min
  ),
  Lambda_1SE = c(
    train_accuracy_1se, auc(train_roc_1se), train_mse_1se,
    test_accuracy_1se, auc(test_roc_1se), test_mse_1se
  )
)

print("Summary of Results:")
results
```

```{r}
fits1 <- glmnet(X_train, Y_train, family = "binomial", type.measure = "class", alpha = 1, lambda = lambda_1se)

# Get predicted probabilities
predicted_probs <- predict(fits1, newx = X_test, type = "response")
predicted_class <- ifelse(predicted_probs >= 0.5, "Yes", "No")
actual_labels <- as.factor(Y_test)
levels(actual_labels) <- c("No", "Yes")

# Create confusion matrix
cm <- confusionMatrix(
  factor(predicted_class),
  factor(actual_labels),
  positive = "Yes"
)

## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[2,1]
FN <- cm$table[1,2]
TP <- cm$table[2,2]

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)

# Create a dataframe for visualization
lasso_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)

# Manually compute the area under ROC curve
roc_curve_lasso <- roc(actual_labels, as.numeric(predictions))
lasso_auc <- auc(roc_curve_lasso)

# ROC curve
plot(roc_curve_lasso, main = "ROC Curve for LASSO", col = "blue", lwd = 2)
```
```{r}
roc_curve_lasso$auc
```


### Splines

We now try a more complicated model for the logistic regression, with the usage of natural splines over the categorical variables, and perform a stepwise regression based on the AIC. [@tibshirani2009elements]

```{r spline, fig.width=12, fig.height=10}
set.seed(23)
fit <- glm(cardio ~ ns(age, df = 4) + ns(height, df = 4) + ns(weight, df = 4) + ns(ap_hi, df = 4) +
              ns(ap_lo, df = 4) + gender + cholesterol + gluc + smoke + alco + active, 
            data = train_data, family = binomial(link = "logit"))

fit_step <- stepAIC(fit, direction = "both", trace = FALSE)

summary(fit_step)

pred <- predict(fit_step, type = "response")
res <- residuals(fit_step, type = "response")
plot(allEffects(fit_step))
```
```{r}
arm::binnedplot(pred, res, nclass = 1000)
```


```{r spline_2}
predicted_probs <- predict(fit_step, newdata = test_data, type = "response")
predicted_class <- ifelse(predicted_probs >= 0.45, "Yes", "No")
actual_labels <- test_data$cardio
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))
cm_table <- as.data.frame(cm$table)
cm_table$Reference <- factor(cm_table$Reference, levels = rev(levels(as.factor(actual_labels))))
colnames(cm_table) <- c("Prediction", "Reference", "Count")

spline_confusion_matrix <- ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Count)) +
  geom_tile() +
  geom_text(aes(label = Count), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Confusion Matrix: Logistic Regression with Splines (threshold = 0.45)", x = "Actual", y = "Predicted")
ggsave("images/spline_confusion_matrix.png", spline_confusion_matrix, width = 8, height = 8)
spline_confusion_matrix

roc_curve_splines <- roc(actual_labels, predicted_probs)
plot(roc_curve_splines, main = "ROC Curve", col = "blue", lwd = 2)
auc_spline <- auc(roc_curve_splines)
print(auc_spline)
png("images/roc_curve_splines.png", width = 800, height = 600)
plot(roc_curve_splines, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()

TN <- cm$table[1, 1]
FP <- cm$table[1, 2]
FN <- cm$table[2, 1]
TP <- cm$table[2, 2]

accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)
fnr_fpr_ratio <- fnr / fpr

splines_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR", "FNR/FPR Ratio"),
  Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr, fnr_fpr_ratio)
)

print(splines_metrics_df)
```


```{r spline decision boundary}
get_mode <- function(x) {
  uniq_x <- unique(x)
  uniq_x[which.max(tabulate(match(x, uniq_x)))]  
}
ap_hi_range <- seq(min(train_data$ap_hi), max(train_data$ap_hi), length.out = 100)
ap_lo_range <- seq(min(train_data$ap_lo), max(train_data$ap_lo), length.out = 100)
grid <- expand.grid(ap_hi = ap_hi_range, ap_lo = ap_lo_range)
grid$age <- median(train_data$age, na.rm = TRUE)
grid$height <- median(train_data$height, na.rm = TRUE)
grid$weight <- median(train_data$weight, na.rm = TRUE)
grid$cholesterol <- get_mode(train_data$cholesterol)
grid$gluc <- get_mode(train_data$gluc)
grid$smoke <- get_mode(train_data$smoke)
grid$alco <- get_mode(train_data$alco)
grid$active <- get_mode(train_data$active)
grid$gender <- get_mode(train_data$gender)
ap_hi_spline <- ns(grid$ap_hi, df = 4)
ap_lo_spline <- ns(grid$ap_lo, df = 4)
grid_splines <- cbind(ap_hi_spline, ap_lo_spline)
grid_full <- cbind(grid, grid_splines)
colnames(grid_full) <- c("ap_hi", "ap_lo", "age", "height", "weight", "cholesterol", "gluc", "smoke", "alco", "active", "gender", paste0("ns_ap_hi", 1:4), paste0("ns_ap_lo", 1:4))
grid_full$gender <- factor(grid_full$gender, levels = levels(train_data$gender))
grid_full$cholesterol <- factor(grid_full$cholesterol, levels = levels(train_data$cholesterol))
grid_full$gluc <- factor(grid_full$gluc, levels = levels(train_data$gluc))
grid_full$smoke <- factor(grid_full$smoke, levels = levels(train_data$smoke))
grid_full$alco <- factor(grid_full$alco, levels = levels(train_data$alco))
grid_full$active <- factor(grid_full$active, levels = levels(train_data$active))

grid_full$predicted_probs <- predict(fit_step, newdata = grid_full, type = "response")
grid_full$predicted_class <- ifelse(grid_full$predicted_probs >= 0.45, "Yes", "No")

# Add new data for "well above normal" cholesterol
grid_well_above <- grid_full
grid_well_above$cholesterol <- "well above normal"
grid_well_above$predicted_probs <- predict(fit_step, newdata = grid_well_above, type = "response")
grid_well_above$predicted_class <- ifelse(grid_well_above$predicted_probs >= 0.45, "Yes", "No")

# Add new data for "above normal" cholesterol
grid_above_normal <- grid_full
grid_above_normal$cholesterol <- "above normal"
grid_above_normal$predicted_probs <- predict(fit_step, newdata = grid_above_normal, type = "response")
grid_above_normal$predicted_class <- ifelse(grid_above_normal$predicted_probs >= 0.45, "Yes", "No")

# Add new data for "normal" cholesterol (this is the default scenario)
grid_normal <- grid_full
grid_normal$cholesterol <- "normal"
grid_normal$predicted_probs <- predict(fit_step, newdata = grid_normal, type = "response")
grid_normal$predicted_class <- ifelse(grid_normal$predicted_probs >= 0.45, "Yes", "No")

ggplot(grid_full, aes(x = ap_hi, y = ap_lo, color = predicted_class)) +
  geom_tile(aes(fill = predicted_class), alpha = 0.3) + 
  geom_point(data = test_data, aes(x = ap_hi, y = ap_lo, color = factor(cardio)), size = 2) + 
  scale_color_manual(values = c("blue", "red")) +  
  scale_fill_manual(values = c("blue", "red")) +
  geom_contour(data = grid_well_above, aes(z = predicted_probs, linetype = "Well Above Normal"), 
               color = "black", linewidth = 1, breaks = 0.45) +
  geom_contour(data = grid_above_normal, aes(z = predicted_probs, linetype = "Above Normal"), 
               color = "gray25", linewidth = 1, breaks = 0.45) +
  geom_contour(data = grid_normal, aes(z = predicted_probs, linetype = "Normal"), 
               color = "gray95", linewidth = 1, breaks = 0.45) +
  labs(title = "Decision Boundary for Logistic Regression Model with Cholesterol Levels (t=0.45)",
       x = "ap_hi", y = "ap_lo", color = "Cardio Outcome", linetype = "Cholesterol Level") +
  scale_linetype_manual(values = c("Normal" = "solid", "Above Normal" = "solid", "Well Above Normal" = "solid")) +
  theme_minimal() +
  theme(legend.position = "bottom",
        legend.box = "horizontal",
        legend.title = element_text(size = 8),  
        legend.text = element_text(size = 6),  
        legend.key.size = unit(0.6, "lines"),   
        legend.key.width = unit(0.8, "lines"), 
        legend.spacing.x = unit(0.5, "mm"),    
        legend.spacing.y = unit(0.5, "mm")) +   
  guides(linetype = guide_legend(ncol = 2))  
```


## Results

Let's compare the results:
```{r}
# Plot the ROC curves
plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
plot(roc_curve_splines, col = "purple", lwd = 2, add = TRUE)

# Add a legend
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4", "Splines"), col = c("red", "blue", "pink", "green", "orange", "purple"), lwd = 2)

# Save image
png("images/roc_results.png", width = 800, height = 600)
plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
plot(roc_curve_splines, col = "purple", lwd = 2, add = TRUE)
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4", "Splines"), col = c("red", "blue", "pink", "green", "orange", "purple"), lwd = 2)
dev.off()

# Model results
results <- data.frame(
  Model = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3", "Polynomial Regression 4", "Splines"),
  AUC = c(
    rf_auc,
    lr_auc,
    lasso_auc,
    auc_poly3,
    auc_poly4,
    auc_spline
  ),
  Sensitivity = c(
    rf_metrics_df$Value[rf_metrics_df$Metric == "Recall (Sensitivity)"],
    lr_metrics_df$Value[lr_metrics_df$Metric == "Recall (Sensitivity)"],
    lasso_metrics_df$Value[lasso_metrics_df$Metric == "Recall (Sensitivity)"],
    poly3_metrics_df$Value[poly3_metrics_df$Metric == "Recall (Sensitivity)"],
    poly4_metrics_df$Value[poly4_metrics_df$Metric == "Recall (Sensitivity)"],
    splines_metrics_df$Value[splines_metrics_df$Metric == "Recall (Sensitivity)"]
  ),
  Specificity = c(
    rf_metrics_df$Value[rf_metrics_df$Metric == "Specificity"],
    lr_metrics_df$Value[lr_metrics_df$Metric == "Specificity"],
    lasso_metrics_df$Value[lasso_metrics_df$Metric == "Specificity"],
    poly3_metrics_df$Value[poly3_metrics_df$Metric == "Specificity"],
    poly4_metrics_df$Value[poly4_metrics_df$Metric == "Specificity"],
    splines_metrics_df$Value[splines_metrics_df$Metric == "Specificity"]
  )
)
print(results)
```

As it is shown above, the five main models get similar results in the task of classifying the data. Despite its slightly better performance, the lack of explainability makes the splines and the random forest less suitable models for this context, leaving the logistic regressions as the best choices.

Regarding the interpretation of the models, and based on the vanilla logistic regression, we can draw some interesting conclusions:

1. On average, having cholesterol *well above normal* means that the difference in the log-odds of the probability of having a heart disease is $1.059$ compared to having a *normal* level. So the odds ratio amounts to $e^{1.059} = 2.88$.

2. In the other hand, being physically active means that the difference in the log-odds of the probability of having a heart disease is $-0.229$ compared to having a *normal* level. So the odds ratio amounts to $e^{-0.229} = 0.80$, what results in a $20\%$ of probability less of having heart issues.

3. An increasing of the systolic or diastolic pressure can also negatively affect your health. For each augment in $10$ units to them (while fixing all the other values), we get odds ratios of $1.76$ and $1.10$ respectively. 


## References

