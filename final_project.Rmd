---
title: "Cardiovascular Disease Data Analysis"
author: Giovanni Billo, Muhammad Mubashar Shahzad, Carlos Velázquez Fernández, Fabio Vicig
date: "2025-01-07"
output: 
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
references:
  - id: narloch1995influence
    title: "Influence of breathing technique on arterial blood pressure during heavy weight lifting"
    author:
      - family: "Narloch"
        given: "Joseph A"
      - family: "Brandstater"
        given: "Murray E"
    container-title: "Archives of physical medicine and rehabilitation"
    volume: 76
    issue: 5
    page: "457-462"
    issued:
      year: 1995
    publisher: "Elsevier"
  - id: cardiovascular_disease_dataset
    author:
      - family: "Ulianova"
        given: "Svetlana"
    title: "Cardiovascular Disease Dataset"
    URL: "https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset"
    accessed:
      year: 2025
      month: 1
      day: 9
    type: dataset
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(cluster)
library(corrplot)
library(dendextend)
library(dplyr)
library(factoextra)
library(ggplot2)
library(gridExtra)
library(patchwork)
library(parallel)
library(pROC)
library(ranger)
library(reshape2)
library(rpart)
library(rpart.plot)
```

## Introduction

The analysis is centered around the prediction of presence of cardiovascular disease, given data about patients [@cardiovascular_disease_dataset]. [TABLE OF CONTENTS, SUMMARY OF WHAT HAS BEEN DONE]

## Data exploration

Firstly, an exploratory analysis has been conducted, with the purpose of identifying problems and have at a glance an idea of meaning and distribution of the variables.

```{r 'Explorative analysis and data cleaning'}
cardio_with_id <- read.csv("data/cardio_train.csv", sep= ";", header=TRUE)
cardio <- cardio_with_id[,-1]
sum(as.numeric(is.na(cardio)))
str(cardio)
summary(cardio)
par(mfrow = c(2,2))
```

The dataset contains 70000 observations of 12 variables. The source doesn't specify how the data was recovered or generated, and it has not been possible to find it elsewhere than Kaggle. The variables are the following:\

- **cardio** : objective variable of our analysis. \
- **age** : age of the patient, in days.\
- **gender** : categorical variable which is 1 for females and 2 for males.\
- **height** : height of the patient, in cm.\
- **weight** : weight of the patient, in kg.\
- **ap_hi** : systolic blood pressure.\
- **ap_lo** : diastolic blood pressure.\
- **cholesterol** : categorical variable with three levels of cholesterol: 1: normal, 2: above normal, 3: well above normal.\
- **gluc** : categorical variable with three levels of glucose: 1: normal, 2: above normal, 3: well above normal.\
- **smoke** : binary variable, representing smokers.\
- **alco** : binary variable, representing alcohol intake.\
- **active** : binary variable, representing physical activity.

The last three variables are marked as "subjective", meaning that this info is supplied by patients themselves (probably through surveying) and therefore, being based on the patient's discretion, is not necessarily true,  \
\
There are no NA values. However, from the summary we can see that there are variables with impossible values: specifically, *ap_hi* and *ap_lo* have in some cases negative values, which don't make sense even for a dead person, and in other cases extremely high values.
The highest pressure recorded in an individual was 370/360 [@narloch1995influence].

```{r}
summary(cardio[(cardio$ap_hi<0|cardio$ap_hi>400|cardio$ap_lo<0|cardio$ap_lo>400|cardio$ap_hi<cardio$ap_lo),])
cardio <- cardio[!(cardio$ap_hi < 0 | cardio$ap_hi > 400 | cardio$ap_lo < 0 | cardio$ap_lo > 400 | cardio$ap_hi < cardio$ap_lo),]

# save a copy of cardio with filtered values but without factor columns for clustering later on
cardio_cl = cardio
```

1274 observations have completely unrealistic values for either systolic or diastolic pressure (or both). In 274 of them, the systolic pressure is less than the diastolic, which is also not possible. These wrong metrics are less than 2% of our dataset, so the most straightforward approach is just to eliminate the problematic rows. \

```{r}
cardio$gender <- factor(cardio$gender)
levels(cardio$gender) = c("F", "M")
head(cardio$gender)

cardio$cardio <- as.factor(cardio$cardio)
head(cardio$cardio)

cardio$cholesterol <- factor(cardio$cholesterol)
levels(cardio$cholesterol) = c("normal", "above normal", "well above normal")
head(cardio$cholesterol)

cardio$gluc <- factor(cardio$gluc)
levels(cardio$gluc) = c("normal", "above normal", "well above normal")
head(cardio$gluc)

cardio$smoke <- factor(cardio$smoke)
levels(cardio$smoke) = c("No", "Yes")
head(cardio$smoke)

cardio$alco <- factor(cardio$alco)
levels(cardio$alco) = c("No", "Yes")
head(cardio$alco)

cardio$active <- factor(cardio$active)
levels(cardio$active) = c("No", "Yes")
head(cardio$active)
```
```{r}
cardio$age = ceiling(cardio$age/365)
min(cardio$age)
```

There are very little observations concerning young people (< 35 years old), and the youngest one in the sample is 30: we therefore split the sample into 2 age categories, senior and elderly.
```{r age_categorizing}
cardio$age_cat <- ifelse(cardio$age <= 55, 'S', 'E')
table(cardio$age_cat)
```

```{r}
cardio$age_cat <- factor(cardio$age_cat)
levels(cardio$age_cat) = c("Senior", "Elderly")
head(cardio$age_cat)
```



```{r}
table(cardio$cardio)
```
The dataset is balanced with respect to the target variable.

Let's divide our dataset between numerical and categorical variables and analyze what are, if any, the relationships between the variables.
```{r eval=TRUE, include=TRUE}
numerical <- cardio[, c("age", "height", "weight", "ap_hi", "ap_lo")]
categorical <- cardio[!(names(cardio) %in% c("age", "height", "weight", "ap_hi", "ap_lo"))]
```

```{r eval=FALSE, include=FALSE}
plot.new()

pairs(numerical,
      labels = colnames(numerical),  
      pch = 21,
      bg = rainbow(2)[(cardio$cardio)],
      col = rainbow(2)[(cardio$cardio)], 
      main = "Cardiovascular disease dataset (numerical variables)",
      row1attop = TRUE,         # If FALSE, changes the direction of the diagonal
      gap = 1,                  # Distance between subplots
      cex.labels = NULL,        # Size of the diagonal text
      font.labels = 1)
# Add the legend
legend(
  "topright",                           # Position of the legend
  legend = c("No disease", "Disease"), # Labels for the categories
  col = rainbow(2),                     # Colors corresponding to categories
  pch = 21,                             # Point shape used in the plot
  pt.bg = rainbow(2),                   # Background color for the points
  bty = "n"                             # No border around the legend box
)
```
Let's visualize the distribution of the numerical variables as well
```{r eval=TRUE}
# Define the number of variables
num_vars <- ncol(numerical)

# Set up a grid layout for larger histograms
par(mfrow = c(ceiling(sqrt(num_vars)), ceiling(sqrt(num_vars)))) # Adjust grid based on number of variables
par(mar = c(4, 4, 2, 1)) # Adjust margins for better spacing

# Loop through each variable and create histograms
for (i in 1:num_vars) {
    hist(
        numerical[[i]], 
        col = rgb(0, 1, 1, alpha = 0.5), 
        main = colnames(numerical)[i], 
        xlab = "Value", 
        ylab = "Frequency",
        border = "white"
    )
}

```

Correlation analysis
```{r}
cor_cardio = round(cor(as.data.frame(numerical)), 2)
melt_cardio = melt(cor_cardio)

ggplot(data = melt_cardio, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), size = 5) +
  scale_fill_gradient2(low = "blue", high = "red",
                       limit = c(-1,1), name="Correlation") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.background = element_blank())
```
The heatmap confirms some (perhaps rather intuitive) relationships:
- Diastolic and systolic blood pressure are heavily correlated
- Both weight and age are positively correlated with higher diastolic and systolic blood pressure, the former slightly more than the latter.

Let's now explore the possible relationships of the other categorical variables with cardio.

```{r}
summary_table = as.data.frame(table(cardio$age_cat, cardio$cardio))
colnames(summary_table) <- c("age_cat", "cardio", "count")

ggplot(summary_table, aes(x = age_cat, y = count, fill = as.factor(cardio))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Counts of Cardio Diseases by Age Category",
    x = "Age Category",
    y = "Count",
    fill = "Cardio"
  ) +
  theme_minimal()
```
We see that, interestingly, a higher occurrence of heart disease occurs in senior patients (from 35 to 55) than in elderly patients: actually the majority of the first category is affected by heart disease, while rather surprisingly the majority of the elderly are not.

We use the Chi-squared test to see to which degree the variables are correlated with cardio.
```{r}
categorical_vars = c("gender", "cholesterol", "gluc", "smoke", "alco", "active", "age_cat")
p_values = c()
for (i in categorical_vars){
  corr = chisq.test(table(cardio$cardio, cardio[, i]))
  p_values[i] = corr$p.value
}
which(p_values < 0.05)
```
We see that all of the variables available, beside gender, seem to be very relevant in determining the insurgence of heart diseases.

Let's now test the mean differences between groups for dystolic and systolic blood pressure.

```{r}
continuous_vars = c("ap_hi", "ap_lo", "weight", "height")
# mean_diff_results <- data.frame(
#     Variable = character(),
#     Group = character(),
#     Test = character(),
#     P_Value = numeric(),
#     stringsAsFactors = FALSE
# )
# t_test = t.test(cardio[, "ap_hi"] ~ cardio[, "smoke"])
# print(t_test)
# 
# anova_test = aov(cardio[, "ap_hi"] ~ cardio[, "gluc"])
test_mean_diff = function(continuous_vars, categorical_vars){
  mean_diff_results <- data.frame(
    Variable = character(),
    Group = character(),
    Test = character(),
    P_Value = numeric(),
    stringsAsFactors = FALSE
)
  for (cont_var in continuous_vars) {
  for (cat_var in categorical_vars) {
    num_levels = length(unique(cardio[, cat_var]))
    if (num_levels == 2) {
      # Perform t-test
      t_test = t.test(cardio[, cont_var] ~ cardio[, cat_var])
      p_value = t_test$p.value
      test_type = "t-test"
    } else {
      # Perform ANOVA
      anova_test = aov(cardio[, cont_var] ~ cardio[, cat_var])
      p_value = summary(anova_test)[[1]]["Pr(>F)"][1]
      test_type = "ANOVA"
    }
    # Append results
    results = data.frame(
                                Variable = cont_var, 
                                Group = cat_var, 
                                Test = test_type,
                                P_Value = p_value
                              )
    mean_diff_results = bind_rows(mean_diff_results, results)
  }
  }
return(mean_diff_results)
  }

# for (cont_var in continuous_vars) {
#   for (cat_var in categorical_vars) {
#     num_levels = length(unique(cardio[, cat_var]))
#     if (num_levels == 2) {
#       # Perform t-test
#       t_test = t.test(cardio[, cont_var] ~ cardio[, cat_var])
#       p_value = t_test$p.value
#       test_type = "t-test"
#     } else {
#       # Perform ANOVA
#       anova_test = aov(cardio[, cont_var] ~ cardio[, cat_var])
#       p_value = summary(anova_test)[[1]]["Pr(>F)"][1]
#       test_type = "ANOVA"
#     }
#     # Append results
#     results = data.frame(
#                                 Variable = cont_var, 
#                                 Group = cat_var, 
#                                 Test = test_type,
#                                 P_Value = p_value
#                               )
#     mean_diff_results = bind_rows(mean_diff_results, results)
#   }
# }
# !!! IDK what is wrong with this loop

# View results
result = test_mean_diff(continuous_vars, categorical_vars)
print(result)
```
Once again, there definitely are some significant differences between the groups considered.
```{r}
# just like in the example, we try to do "visual" clustering, see if there are any evident clusters by plotting data
p1 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = cholesterol))
p2 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = age_cat))
p3 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = smoke))
p4 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = active))
p5 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = alco))
p6 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = gluc))
```



```{r}
grid.arrange(p1, p2, p3, p4) #NOTE: TOO MANY TO VISUALIZE WELL, SO JITTER COULD BE USED!!!
```

```{r}
grid.arrange(p5,p6)
```
```{r}
p8 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = cholesterol))
p9 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = age_cat))
p10 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = smoke))
p11 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = active))
p12 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = alco))
p13 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = gluc))

grid.arrange(p8, p9, p10, p11)
```

```{r}
grid.arrange(p12, p13)
```
While no group shows a clean division with respect to `cardio`, values of both `gluc` and `cholesterol` above normal or well above normal seem to be particularly frequent along with the occurrence of heart diseases.

```{r barplots}
cholesterol_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, cholesterol = cardio$cholesterol), 
  FUN = sum
)

cholesterol_data$total <- ave(cholesterol_data$count, cholesterol_data$cardio, FUN = sum)
cholesterol_data$percentage <- (cholesterol_data$count / cholesterol_data$total) * 100

pChol <- ggplot(cholesterol_data, aes(x = as.factor(cardio), y = count, fill = cholesterol)) +
  geom_bar(stat = "identity", position = "stack") +  # Stacked bars based on count
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Population by Cholesterol \nand disease presence", x = expression(paste("\u2003\u2003","Cardio (0 = No Disease, 1 = Disease)")), y = "Count", fill = "Cholesterol") +
  theme(plot.margin = margin(1, 1, 1, 2))

glucose_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, gluc = cardio$gluc), 
  FUN = sum
)

glucose_data$total <- ave(glucose_data$count, glucose_data$cardio, FUN = sum)
glucose_data$percentage <- (glucose_data$count / glucose_data$total) * 100

pGluc <- ggplot(glucose_data, aes(x = as.factor(cardio), y = count, fill = gluc)) +
  geom_bar(stat = "identity", position = "stack") +  # Stacked bars based on count
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(
    title = "Population by Glucose \nand disease presence",
    x = "Cardio (0 = No Disease, 1 = Disease)",
    y = "Count",
    fill = "Glucose"
  ) +
  theme_minimal()
grid.arrange(pChol, pGluc,ncol = 2)
```

## Clustering
### K-means clustering
```{r}
# Scale the data to normalize the values
cardio_data_scaled <- scale(cardio_cl[, -12])

# View the first few rows of the dataset
head(cardio_data_scaled)

# Perform K-means clustering
set.seed(123)  
silhouette_scores = c()

# Compute the distance matrix for the scaled data
sample_cardio_data_scaled = sample(cardio_data_scaled, 10000)
dist_matrix <- dist(sample_cardio_data_scaled)
# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)
# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_clusters in 2:10) {
   # Perform k-means clustering
   kmeans_result <- kmeans(sample_cardio_data_scaled, centers = n_clusters)

   # Extract cluster labels from the k-means result
   cluster_labels <- kmeans_result$cluster

   # Calculate silhouette values
   sil <- silhouette(cluster_labels, dist_matrix)

   # Store the average silhouette score for the current number of clusters
   silhouette_scores[n_clusters] <- mean(sil[, 3])
 }

max_silhouette_kmeans = which.max(silhouette_scores)

best_kmeans_result <- kmeans(cardio_data_scaled, centers = max_silhouette_kmeans)
# Add the cluster assignment to the original data
cardio$Cluster <- as.factor(best_kmeans_result$cluster)
str(best_kmeans_result)
fviz_cluster(best_kmeans_result, data = cardio_data_scaled)
```

```{r}


# Visualize the clusters across different dimensions
cl_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
cl_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(cl_p1, cl_p2)
```
Plotting along the cardio and both types of pressures, the clusters seem to neatly separate the data along some pressure values. 
```{r}
cl_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(cl_p3, cardio_pl)
```
Plotting `ap_hi` against `ap_lo`, the division is even more evident: and it somehow resembles the boundaries described by the `cardio` variable itself.

Let's test for mean differences across clusters
```{r}
info_cluster1 = cardio %>% filter(Cluster == 1)
summary(info_cluster1)
```
```{r}
info_cluster2 = cardio %>% filter(Cluster == 2)
summary(info_cluster2)
```
It seems there are way more individuals with higher values of `cholesterol` and `gluc` in the second cluster. Furthermore, here seniors are the majority, while the first one consists mostly of elderly individuals.
```{r}
cluster_differences = c()
for (i in continuous_vars){
  wilcox_test_result <- wilcox.test(ap_hi ~ Cluster, data = cardio)
  cluster_differences[i] = wilcox_test_result$p.value
  print(wilcox_test_result$p.value)
}

for (i in categorical_vars){
  # Create a contingency table
  contingency_table <- table(cardio[[i]], cardio$Cluster)

  # Perform the Chi-Square test
  chi_square_result <- chisq.test(contingency_table)
  cluster_differences[i] = chi_square_result$p.value
  print(chi_square_result$p.value)
}

```
The clusters are very different across all metrics, not only those we supposed.
Summarizing our result, there seems to be a "healthier" group (cluster 1), where there are less people with high values of `cholesterol` and `gluc`, less drinkers, more active people, more elderly individuals and, finally, more females. 
Roughly 75% of cluster 1 doesn't have heart diseases, while 87 % of cluster 2 does.

### Hierarchical clustering
```{r}
# Calculate the distance matrix
dist_matrix <- dist(sample(cardio_data_scaled, 10000))

# Perform hierarchical clustering
hclust_result <- hclust(dist_matrix, method = "ward.D2")

silhouette_scores = c()
# Compute silhouette
for (n_clusters in 2:10){
  cluster_labels <- cutree(hclust_result, k = n_clusters)
  sil <- silhouette(cluster_labels, dist_matrix)
  silhouette_scores[n_clusters] = mean(sil[, 3])
}

best_n_cluster = which.max(silhouette_scores)

# visualize the best performing cluster
dend <- as.dendrogram(hclust_result)

# Color branches by clusters
dend <- color_branches(dend, k = 3)

# Plot the dendrogram
plot(dend, main = "Hierarchical Clustering with Colored Clusters")
```

```{r}
# Plot silhouette (!!! IDK what is wrong with this as well)
best_cluster_labels <- cutree(hclust_result, k = best_n_cluster)
best_sil <- silhouette(best_cluster_labels, dist_matrix)
plot(best_sil, main = "Best-performing Silhouette Plot for Clustering")
```

We now augment our dataset with the cluster group. 
```{r}
cardio_data_scaled$cluster <- best_cluster_labels

# View the dataset with cluster labels
head(cardio_data_scaled)
```



## Classification Trees

```{r classification trees (high cp)}
# Split data (train/test)
set.seed(123)
cardio_cl$cardio <- as.factor(cardio_cl$cardio)
train_index <- sample(1:nrow(cardio_cl), 0.8 * nrow(cardio_cl))  
train_data <- cardio_cl[train_index, ]
test_data <- cardio_cl[-train_index, ]

# Build and plot the classification tree
tree <- rpart(cardio ~ ., data = train_data, method = "class")
rpart.plot(tree)

# Check feature importance
tree$variable.importance

# Make predictions
predictions <- predict(tree, test_data, type = "prob")

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions[, 2] > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)
confusion_matrix

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# ROC curve
roc_curve <- roc(test_data$cardio, predictions[, 2])
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))
```
At first sight, we can observe that the classification tree completely relies on the feature `ap_hi` for the only split. After checking the importance of the difference features, this one, together with `ap_lo`, out stand over the others. Even though both of them are significant, they are highly correlated, so taking them into account at the same time would introduce redundancy in the tree.

Let's check the control parameters in the building of the model:
```{r}
# Control parameters
tree$control
```
Could we improve the performance of the model with more splits? In order to force the tree to use less relevant features, we will readjust some of this parameters. Specifically, we will reduce the complexity parameter (`cp`).
```{r classification trees (low cp)}
# New classification tree
tree <- rpart(cardio ~ ., data = train_data, method = "class",
              control = rpart.control(cp = 0.001))
rpart.plot(tree)

# Check feature importance
tree$variable.importance

# Make predictions
predictions <- predict(tree, test_data, type = "prob")

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions[, 2] > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)
confusion_matrix

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# ROC curve
roc_curve <- roc(test_data$cardio, predictions[, 2])
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))
```

After evaluating the model, we get a sightly better accuracy from a significantly more complex tree. Moreover, the area under the ROC curve also increases (from 0.7056 to 0.7770). From this comparison, we can conclude that although the blood pressure is one the best indicators (in this dataset) of cardiovascular diseases, observing other variables like cholesterol, weight, age or glucose also helps to identify them.


## Random Forest

Following classification trees, a natural approximation is fitting a random forest model. Let's keep the default parameters:

- `num.trees = 500`: the number of decision trees.
- `mtry = sqrt(F)`: the number of features to use in each decision tree, where *F* is the total number of features.

```{r random forest}
# Random forest
rf_model <- ranger(cardio ~ ., data = train_data, probability = TRUE, num.threads = detectCores())
rf_model

# Predict on test data
predictions <- predict(rf_model, data = test_data)

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions$predictions[, 2] > 0.5, 1, 0)
predicted_classes <- as.factor(predicted_classes)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
```
Let's check the area under the ROC curve:

```{r}
# Compute the ROC curve
roc_curve <- roc(test_data$cardio, predictions$predictions[, 2])

# Compute the AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))

plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```

As we could imagine, this enhanced version of classification trees gets a better performance in the classification task. However, we must take into account the high computational complexity of this new model compared with the previous ones.


## References
