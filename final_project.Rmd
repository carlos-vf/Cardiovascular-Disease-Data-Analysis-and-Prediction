---
title: "Cardiovascular Disease Data Analysis"
author: Giovanni Billo, Muhammad Mubashar Shahzad, Carlos Velázquez Fernández, Fabio Vicig
date: "2025-01-07"
output: 
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
    
references:
  
  - id: narloch1995influence
    title: "Influence of breathing technique on arterial blood pressure during heavy weight lifting"
    author:
      - family: "Narloch"
        given: "Joseph A"
      - family: "Brandstater"
        given: "Murray E"
    container-title: "Archives of physical medicine and rehabilitation"
    volume: 76
    issue: 5
    page: "457-462"
    issued:
      year: 1995
    publisher: "Elsevier"
    
  - id: breiman1984classification
    title: "Classification and Regression Trees"
    author:
      - family: "Breiman"
        given: "L."
      - family: "Friedman"
        given: "J."
      - family: "Olshen"
        given: "R.A."
      - family: "Stone"
        given: "C.J."
    edition: "1st"
    publisher: "Chapman and Hall/CRC"
    issued:
      year: 1984
    URL: "https://doi.org/10.1201/9781315139470"
    DOI: "10.1201/9781315139470"

    
  - id: cardiovascular_disease_dataset
    author:
      - family: "Ulianova"
        given: "Svetlana"
    title: "Cardiovascular Disease Dataset"
    URL: "https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset"
    accessed:
      year: 2025
      month: 1
      day: 9
    type: dataset
    
  - id: bertsimas2017optimal
    title: "Optimal classification trees"
    author:
      - family: "Bertsimas"
        given: "Dimitris"
      - family: "Dunn"
        given: "Jack"
    container-title: "Machine Learning"
    volume: 106
    issue: 7
    page: "1039-1082"
    issued:
      year: 2017
      month: 7
      day: 1
    publisher: "Springer"
    URL: "https://doi.org/10.1007/s10994-017-5633-9"
    DOI: "10.1007/s10994-017-5633-9"
  
  - id: breiman2001random
    title: "Random Forests"
    author:
      - family: "Breiman"
        given: "Leo"
    container-title: "Machine Learning"
    volume: 45
    issue: 1
    page: "5-32"
    issued:
      year: 2001
      month: 10
      day: 1
    URL: "https://doi.org/10.1023/A:1010933404324"
    DOI: "10.1023/A:1010933404324"

  - id: hastie2009elements
    title: "The Elements of Statistical Learning"
    author:
      - family: "Hastie"
        given: "Trevor"
      - family: "Tibshirani"
        given: "Robert"
      - family: "Friedman"
        given: "Jerome"
    edition: "2nd"
    series-title: "Springer Series in Statistics"
    publisher: "Springer New York, NY"
    issued:
      year: 2009
      month: 8
      day: 26
    ISBN:
      - "978-0-387-84857-0"
      - "978-0-387-84858-7"
    DOI: "https://doi.org/10.1007/978-0-387-84858-7"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

The analysis is centered around the prediction of presence of cardiovascular disease, given data about patients [@cardiovascular_disease_dataset]. [TABLE OF CONTENTS, SUMMARY OF WHAT HAS BEEN DONE]


## Importing libraries
```{r libraries}
suppressMessages(library(caret))
suppressMessages(library(cluster))
suppressMessages(library(corrplot))
suppressMessages(library(dendextend))
suppressMessages(library(doParallel))
suppressMessages(library(dplyr))
suppressMessages(library(factoextra))
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
suppressMessages(library(patchwork))
suppressMessages(library(parallel))
suppressMessages(library(pROC))
suppressMessages(library(ranger))
suppressMessages(library(reshape2))
suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
```



## Data exploration

Firstly, an exploratory analysis has been conducted, with the purpose of identifying problems and have at a glance an idea of meaning and distribution of the variables.

```{r 'Explorative analysis and data cleaning'}
cardio_with_id <- read.csv("data/cardio_train.csv", sep= ";", header=TRUE)
cardio <- cardio_with_id[,-1]
sum(as.numeric(is.na(cardio)))
str(cardio)
summary(cardio)
par(mfrow = c(2,2))
```

The dataset contains 70000 observations of 12 variables. The source doesn't specify how the data was recovered or generated, and it has not been possible to find it elsewhere than Kaggle. The variables are the following:\

- **cardio** : objective variable of our analysis. It is a binary variable representing whether cardiovascular disease is present in the patient. \
- **age** : age of the patient, in days.\
- **gender** : categorical variable which is 1 for females and 2 for males.\
- **height** : height of the patient, in cm.\
- **weight** : weight of the patient, in kg.\
- **ap_hi** : systolic blood pressure.\
- **ap_lo** : diastolic blood pressure.\
- **cholesterol** : categorical variable with three levels of cholesterol: 1: normal, 2: above normal, 3: well above normal.\
- **gluc** : categorical variable with three levels of glucose: 1: normal, 2: above normal, 3: well above normal.\
- **smoke** : binary variable, representing smokers.\
- **alco** : binary variable, representing alcohol intake.\
- **active** : binary variable, representing physical activity.

The last three variables are marked as "subjective", meaning that this piece of information is likely supplied by patients themselves, possibly using different metrics.\
\
There are no NA values. However, from the summary we can see that there are variables with impossible values: specifically, *ap_hi* and *ap_lo* have in some cases negative values, which don't make sense even for a dead person, and in other cases extremely high values. A cut-off value value for the blood pressure is selected (generously) by basing on a study that measured blood pressure during intense exercise, and found a maximum pressure of 370/360 while using the Valsalva and a maximum of 250/230 without it 
[@narloch1995influence]. 

```{r}
summary(cardio[(cardio$ap_hi<0|cardio$ap_hi>400|cardio$ap_lo<0|cardio$ap_lo>400|cardio$ap_hi<cardio$ap_lo),])
cardio <- cardio[!(cardio$ap_hi < 0 | cardio$ap_hi > 400 | cardio$ap_lo < 0 | cardio$ap_lo > 400 | cardio$ap_hi < cardio$ap_lo | cardio$ap_hi-cardio$ap_lo > 250),]
```

1274 observations have completely unrealistic values for either systolic or diastolic pressure (or both). In 274 of them, the systolic pressure is less than the diastolic, which is also not possible. These wrong metrics are less than 2% of the dataset, so the most straightforward approach is just to eliminate the problematic rows. \

```{r}
cardio$gender <- factor(cardio$gender)
levels(cardio$gender) = c("F", "M")
#head(cardio$gender)

cardio$cardio <- as.factor(cardio$cardio)
#head(cardio$cardio)

cardio$cholesterol <- factor(cardio$cholesterol)
levels(cardio$cholesterol) = c("normal", "above normal", "well above normal")
#head(cardio$cholesterol)

cardio$gluc <- factor(cardio$gluc)
levels(cardio$gluc) = c("normal", "above normal", "well above normal")
#head(cardio$gluc)

cardio$smoke <- factor(cardio$smoke)
levels(cardio$smoke) = c("No", "Yes")
#head(cardio$smoke)

cardio$alco <- factor(cardio$alco)
levels(cardio$alco) = c("No", "Yes")
#head(cardio$alco)

cardio$active <- factor(cardio$active)
levels(cardio$active) = c("No", "Yes")
#head(cardio$active)
```
```{r}
table(cardio$cardio)
```

The dataset is balanced with respect to the target variable.\
Dividing age by 365 to obtain it in years:

```{r}
cardio$age = ceiling(cardio$age/365)
summary(cardio$age)
ggplot(cardio, aes(y = age)) +
  geom_boxplot(fill = "skyblue", color = "black") + labs(
    y = "Age"                                       
  ) +
  theme_minimal()   
nrow(cardio[cardio$age<40,])
# saving a copy of cardio with filtered values but without factor columns for clustering later on
cardio_cl <- cardio
```

There are only 4 observations concerning people < 40 years old, and the youngest one in the sample is 30: for this reason, we split between under (or equal to) and over 55.

```{r age_categorizing}
cardio$age_cat <- ifelse(cardio$age <= 55, '30-55', '55-65')
table(cardio$age_cat)
```

```{r}
cardio$age_cat <- factor(cardio$age_cat)
#levels(cardio$age_cat) = c("Senior", "Elderly")
levels(cardio$age_cat) = c("Under 55", "Over 55")
```


Numerical and categorical variables are specified for convenience:

```{r eval=TRUE, include=TRUE}
numerical <- cardio[, c("age", "height", "weight", "ap_hi", "ap_lo")]
categorical <- cardio[!(names(cardio) %in% c("age", "height", "weight", "ap_hi", "ap_lo"))]
```

```{r eval=TRUE}
num_vars <- ncol(numerical)
par(mfrow = c(ceiling(sqrt(num_vars)), floor(sqrt(num_vars)))) 
for (i in 1:num_vars) {
    hist(
        numerical[[i]], col = "skyblue",main=colnames(numerical)[i], xlab = "Value", ylab = "Frequency", border = "black"   )
}

```

We now proceed to plot the distribution of the numerical variables, with respect to the disease status:

```{r}
library(GGally)
library(ggplot2)

gdens1<- ggplot(cardio, aes(x = ap_hi, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 4) +  
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) +
  labs(fill = "Cardiovascular disease", x = "Systolic pressure") +  
  theme_minimal() +  
  theme(legend.position = "top")
gdens2<-ggplot(cardio, aes(x = ap_lo, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 4) +  
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Diastolic pressure") +  
  theme_minimal() +  
  theme(legend.position = "top")
gdens3<-ggplot(cardio, aes(x = ap_hi-ap_lo, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 4) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Pulse pressure") + 
  theme_minimal() +
  theme(legend.position = "top")
gdens4<-ggplot(cardio, aes(x = height, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 2) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Height") + 
  theme_minimal() +
  theme(legend.position = "top")
gdens5<-ggplot(cardio, aes(x = weight, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 2) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Weight") + 
  theme_minimal() +
  theme(legend.position = "top")
gdens6<-ggplot(cardio, aes(x = age, fill = factor(cardio))) +
  geom_density(alpha = 0.5,bw = 1) + 
  scale_fill_manual(values = c("0" = "royalblue", "1" = "lightcoral")) + 
  labs(fill = "Cardiovascular disease", x = "Age") + 
  theme_minimal() +
  theme(legend.position = "top")
grid.arrange(gdens1,gdens2, nrow = 2)
grid.arrange(gdens3,gdens6, nrow = 2)
grid.arrange(gdens4,gdens5, nrow = 2)
```


Correlation analysis

```{r}
cor_cardio = round(cor(as.data.frame(numerical)), 2)
melt_cardio = melt(cor_cardio)

ggplot(data = melt_cardio, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), size = 5) +
  scale_fill_gradient2(low = "blue", high = "red",
                       limit = c(-1,1), name="Correlation") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.background = element_blank())
```

The heatmap confirms some (perhaps rather intuitive) relationships:\
- Diastolic and systolic blood pressure are heavily correlated\
- Both weight and age are positively correlated with higher diastolic and systolic blood pressure, the former slightly more than the latter.\

Let's now explore the possible relationships of the categorical variables with cardio.

```{r barplots}

stacked_bplot <- function(data, x, fill, fill_colors, title, x_label, y_label, fill_label) {
  data <- aggregate(x = list(count = rep(1, nrow(data))), by = list(cardio = data$cardio, group = data[[fill]]), FUN = sum)
  data$total <- ave(data$count, data$cardio, FUN = sum)
  data$percentage <- (data$count / data$total) * 100
  ggplot(data, aes(x = as.factor(cardio), y = count, fill = as.factor(group))) +
    geom_bar(stat = "identity", position = "stack") +
    geom_text(aes(label = paste0(round(percentage, 1), "%")), position = position_stack(vjust = 0.5), size = 3) +
    labs(title = title, x = x_label, y = y_label, fill = fill_label) +
    scale_fill_manual(values = fill_colors) +
    theme_minimal()
}

pChol <- stacked_bplot(cardio, "cardio", "cholesterol", c("royalblue", "gold", "lightcoral"),
  "Population by Cholesterol \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Cholesterol"
)

pGluc <- stacked_bplot(cardio, "cardio", "gluc", c("royalblue", "gold", "lightcoral"),
  "Population by Glucose \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Glucose"
)

pSmoke <- stacked_bplot(cardio, "cardio", "smoke", c("royalblue", "lightcoral"),
  "Population by Smoking Status \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Smoking"
)

pActive <- stacked_bplot(cardio, "cardio", "active", c("lightcoral", "royalblue"),
  "Population by Physical Activity \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Active"
)

pAlco <- stacked_bplot(cardio, "cardio", "alco", c("royalblue", "lightcoral"),
  "Population by Alcohol Consumption \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Alcohol"
)

pGender <- stacked_bplot(cardio, "cardio", "gender", c("lightcoral", "royalblue"),
  "Population by Gender \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Gender"
)

pAgeCat <- stacked_bplot(cardio, "cardio", "age_cat", c("royalblue", "lightcoral"),
  "Population by Age Category \nand disease presence", "Cardio (0 = No Disease, 1 = Disease)", "Count", "Age Category"
)

grid.arrange(pChol, pGluc, ncol = 2)
grid.arrange(pSmoke, pActive, ncol = 2)
grid.arrange(pAlco, pGender, ncol = 2)
grid.arrange(pAgeCat, ncol = 2)

```

As expected, cardiovascular diseases are more common in the older age group.\
We use the Chi-squared test to see to which degree the variables are correlated with cardio:

```{r}
categorical_vars <- c("gender", "cholesterol", "gluc", "smoke", "alco", "active", "age_cat")
test_results <- list()

for (var in categorical_vars) {
  test <- chisq.test(table(cardio$cardio, cardio[[var]]))
  test_results[[var]] <- test
}

for (var in categorical_vars) {
  cat("Chi-Squared Test for", var, ":\n")
  print(test_results[[var]])
  cat("\n")
}

significant_vars <- names(test_results)[sapply(test_results, function(x) x$p.value < 0.05)]
cat("Variables with significant association (p < 0.05):\n")
print(significant_vars)
```

We see that all of the available categorical variables, besides gender, seem to be very relevant in determining the insurgence of heart diseases.

Let's now test the mean differences between groups for dystolic and systolic blood pressure.

```{r}
continuous_vars = c("ap_hi", "ap_lo", "weight", "height", "age")
# mean_diff_results <- data.frame(
#     Variable = character(),
#     Group = character(),
#     Test = character(),
#     P_Value = numeric(),
#     stringsAsFactors = FALSE
# )
# t_test = t.test(cardio[, "ap_hi"] ~ cardio[, "smoke"])
# print(t_test)
# 
# anova_test = aov(cardio[, "ap_hi"] ~ cardio[, "gluc"])
test_mean_diff = function(continuous_vars, categorical_vars){
  mean_diff_results <- data.frame(
    Variable = character(),
    Group = character(),
    Test = character(),
    P_Value = numeric(),
    stringsAsFactors = FALSE
)
  for (cont_var in continuous_vars) {
  for (cat_var in categorical_vars) {
    num_levels = length(unique(cardio[, cat_var]))
    if (num_levels == 2) {
      # Perform t-test
      t_test = t.test(cardio[, cont_var] ~ cardio[, cat_var])
      p_value = t_test$p.value
      test_type = "t-test"
    } else {
      # Perform ANOVA
      anova_test = aov(cardio[, cont_var] ~ cardio[, cat_var])
      p_value = summary(anova_test)[[1]]["Pr(>F)"][1]
      test_type = "ANOVA"
    }
    # Append results
    results = data.frame(
                                Variable = cont_var, 
                                Group = cat_var, 
                                Test = test_type,
                                P_Value = p_value
                              )
    mean_diff_results = bind_rows(mean_diff_results, results)
  }
  }
return(mean_diff_results)
  }

# for (cont_var in continuous_vars) {
#   for (cat_var in categorical_vars) {
#     num_levels = length(unique(cardio[, cat_var]))
#     if (num_levels == 2) {
#       # Perform t-test
#       t_test = t.test(cardio[, cont_var] ~ cardio[, cat_var])
#       p_value = t_test$p.value
#       test_type = "t-test"
#     } else {
#       # Perform ANOVA
#       anova_test = aov(cardio[, cont_var] ~ cardio[, cat_var])
#       p_value = summary(anova_test)[[1]]["Pr(>F)"][1]
#       test_type = "ANOVA"
#     }
#     # Append results
#     results = data.frame(
#                                 Variable = cont_var, 
#                                 Group = cat_var, 
#                                 Test = test_type,
#                                 P_Value = p_value
#                               )
#     mean_diff_results = bind_rows(mean_diff_results, results)
#   }
# }
# !!! IDK what is wrong with this loop

# View results
result = test_mean_diff(continuous_vars, categorical_vars)
print(result)
```

Once again, there definitely are some significant differences between the groups considered.

```{r}
# just like in the example, we try to do "visual" clustering, see if there are any evident clusters by plotting data
p1 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = cholesterol))
p2 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = age_cat))
p3 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = smoke))
p4 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = active))
p5 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = alco))
p6 <- ggplot(cardio, aes(cardio, ap_hi)) + geom_point(aes(color = gluc))
```


```{r}
grid.arrange(p1, p2, p3, p4)
```

```{r}
grid.arrange(p5,p6)
```

```{r}
p8 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = cholesterol))
p9 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = age_cat))
p10 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = smoke))
p11 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = active))
p12 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = alco))
p13 <- ggplot(cardio, aes(cardio, ap_lo)) + geom_point(aes(color = gluc))

grid.arrange(p8, p9, p10, p11)
```

```{r}
grid.arrange(p12, p13)
```

While no group shows a clean division with respect to `cardio`, values of both `gluc` and `cholesterol` above normal or well above normal seem to be particularly frequent along with the occurrence of heart diseases.

```{r barplots}
#Cholesterol
cholesterol_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, cholesterol = cardio$cholesterol), 
  FUN = sum
)

cholesterol_data$total <- ave(cholesterol_data$count, cholesterol_data$cardio, FUN = sum)
cholesterol_data$percentage <- (cholesterol_data$count / cholesterol_data$total) * 100

pChol <- ggplot(cholesterol_data, aes(x = as.factor(cardio), y = count, fill = cholesterol)) +
  geom_bar(stat = "identity", position = "stack") + 
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Population by Cholesterol \nand disease presence", x = expression(paste("\u2003\u2003","Cardio (0 = No Disease, 1 = Disease)")), y = "Count", fill = "Cholesterol") + scale_fill_manual(values = c("royalblue", "gold", "lightcoral")) +
  theme(plot.margin = margin(1, 1, 1, 2))

#Glucose
glucose_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, gluc = cardio$gluc), 
  FUN = sum
)

glucose_data$total <- ave(glucose_data$count, glucose_data$cardio, FUN = sum)
glucose_data$percentage <- (glucose_data$count / glucose_data$total) * 100

pGluc <- ggplot(glucose_data, aes(x = as.factor(cardio), y = count, fill = gluc)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(
    title = "Population by Glucose \nand disease presence",
    x = "Cardio (0 = No Disease, 1 = Disease)",
    y = "Count",
    fill = "Glucose"
  ) + scale_fill_manual(values = c("royalblue", "gold", "lightcoral")) +
  theme_minimal()  

# Smoke
smoke_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, smoke = cardio$smoke), 
  FUN = sum
)

smoke_data$total <- ave(smoke_data$count, smoke_data$cardio, FUN = sum)
smoke_data$percentage <- (smoke_data$count / smoke_data$total) * 100

pSmoke <- ggplot(smoke_data, aes(x = as.factor(cardio), y = count, fill = smoke)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(
    title = "Population by Smoking Status \nand disease presence",
    x = "Cardio (0 = No Disease, 1 = Disease)",
    y = "Count",
    fill = "Smoking"
  ) +  scale_fill_manual(values = c("royalblue","lightcoral")) +
  theme_minimal()

# Active
active_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, active = cardio$active), 
  FUN = sum
)

active_data$total <- ave(active_data$count, active_data$cardio, FUN = sum)
active_data$percentage <- (active_data$count / active_data$total) * 100

pActive <- ggplot(active_data, aes(x = as.factor(cardio), y = count, fill = active)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(
    title = "Population by Physical Activity \nand disease presence",
    x = "Cardio (0 = No Disease, 1 = Disease)",
    y = "Count",
    fill = "Active"
  ) + scale_fill_manual(values = c("royalblue","lightcoral")) +
  theme_minimal()

# Alcohol
alco_data <- aggregate(
  x = list(count = rep(1, nrow(cardio))), 
  by = list(cardio = cardio$cardio, alco = cardio$alco), 
  FUN = sum
)

alco_data$total <- ave(alco_data$count, alco_data$cardio, FUN = sum)
alco_data$percentage <- (alco_data$count / alco_data$total) * 100

pAlco <- ggplot(alco_data, aes(x = as.factor(cardio), y = count, fill = alco)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(
    title = "Population by Alcohol Consumption \nand disease presence",
    x = "Cardio (0 = No Disease, 1 = Disease)",
    y = "Count",
    fill = "Alcohol"
  ) +  scale_fill_manual(values = c("royalblue","lightcoral")) +
  theme_minimal()

grid.arrange(pChol, pGluc,ncol = 2)
grid.arrange(pSmoke, pActive, ncol = 2)
grid.arrange(pAlco, ncol = 2)
```



## Clustering
### K-means clustering (old, scale everything-> 2 clusters)
```{r}
# Scale the data to normalize the values
cardio_data_scaled <- scale(cardio_cl[, -12])

# View the first few rows of the dataset
head(cardio_data_scaled)
```

```{r}
# Compute the distance matrix for the scaled data
sample_cardio_data_scaled = sample(cardio_data_scaled, 10000)
dist_matrix <- dist(sample_cardio_data_scaled)
head(dist_matrix)
```


```{r}
# Perform K-means clustering
set.seed(123)  
silhouette_scores = c()

# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)
# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_clusters in 2:10) {
   # Perform k-means clustering
   kmeans_result <- kmeans(sample_cardio_data_scaled, centers = n_clusters)

   # Extract cluster labels from the k-means result
   cluster_labels <- kmeans_result$cluster

   # Calculate silhouette values
   sil <- silhouette(cluster_labels, dist_matrix)

   # Store the average silhouette score for the current number of clusters
   silhouette_scores[n_clusters] <- mean(sil[, 3])
 }

max_silhouette_kmeans = which.max(silhouette_scores)

best_kmeans_result <- kmeans(cardio_data_scaled, centers = max_silhouette_kmeans)

# Add the cluster assignment to the original data
cardio$Cluster <- as.factor(best_kmeans_result$cluster)
str(best_kmeans_result)
```

```{r}
# Visualize the clusters across different dimensions
cl_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
cl_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(cl_p1, cl_p2)
```
Plotting along the cardio and both types of pressures, the clusters seem to neatly separate the data along some pressure values. 
```{r}
cl_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")

cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(cl_p3, cardio_pl)
```
Plotting `ap_hi` against `ap_lo`, the division is even more evident: and it somehow resembles the boundaries described by the `cardio` variable itself.

Let's test for mean differences across clusters
```{r}
info_cluster1 = cardio %>% filter(Cluster == 1)
summary(info_cluster1)
```
```{r}
info_cluster2 = cardio %>% filter(Cluster == 2)
summary(info_cluster2)
```
It seems there are way more individuals with higher values of `cholesterol` and `gluc` in the second cluster. Furthermore, here seniors are the majority, while the first one consists mostly of elderly individuals.
```{r}
cluster_differences = c()
for (i in continuous_vars){
  wilcox_test_result <- wilcox.test(ap_hi ~ Cluster, data = cardio)
  cluster_differences[i] = wilcox_test_result$p.value
  print(wilcox_test_result$p.value)
}

for (i in categorical_vars){
  # Create a contingency table
  contingency_table <- table(cardio[[i]], cardio$Cluster)

  # Perform the Chi-Square test
  chi_square_result <- chisq.test(contingency_table)
  cluster_differences[i] = chi_square_result$p.value
  print(chi_square_result$p.value)
}

```
The clusters are very different across all metrics, not only those we supposed.
Summarizing our result, there seems to be a "healthier" group (cluster 1), where there are less people with high values of `cholesterol` and `gluc`, less drinkers, more active people, more elderly individuals and, finally, more females. 
Roughly 75% of cluster 1 doesn't have heart diseases, while 87 % of cluster 2 does.



### K-means clustering (scale only numerical ->)
```{r}
# Filter the dataset to exclude these columns and scale the remaining numeric columns
cardio_cl[, c(continuous_vars, "cardio")] <- scale(cardio_cl[, c(continuous_vars, "cardio")])
cardio_data_scaled <- cardio_cl

# View the first few rows of the dataset
head(cardio_data_scaled)
```

```{r}
# Compute the distance matrix for the scaled data
sample_cardio_data_scaled = sample(as.matrix(cardio_data_scaled), 10000)
dist_matrix <- dist(sample_cardio_data_scaled)
head(dist_matrix)
```


```{r}
# Perform K-means clustering
set.seed(123)  
silhouette_scores = c()

# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)
# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_clusters in 2:10) {
   # Perform k-means clustering
   kmeans_result <- kmeans(sample_cardio_data_scaled, centers = n_clusters)

   # Extract cluster labels from the k-means result
   cluster_labels <- kmeans_result$cluster

   # Calculate silhouette values
   sil <- silhouette(cluster_labels, dist_matrix)

   # Store the average silhouette score for the current number of clusters
   silhouette_scores[n_clusters] <- mean(sil[, 3])
 }

max_silhouette_kmeans = which.max(silhouette_scores)

best_kmeans_result <- kmeans(cardio_data_scaled, centers = max_silhouette_kmeans)
# Add the cluster assignment to the original data
cardio$Cluster <- as.factor(best_kmeans_result$cluster)
str(best_kmeans_result)
```

```{r}
# Visualize the clusters across different dimensions
cl_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
cl_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(cl_p1, cl_p2)
```
Plotting along the cardio and both types of pressures, the clusters seem to neatly separate the data along some pressure values. 
```{r}
cl_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(cl_p3, cardio_pl)
```
Plotting `ap_hi` against `ap_lo`, the division is even more evident: and it somehow resembles the boundaries described by the `cardio` variable itself.

Let's test for mean differences across clusters
```{r}
info_cluster1 = cardio %>% filter(Cluster == 1)
summary(info_cluster1)
```
```{r}
info_cluster2 = cardio %>% filter(Cluster == 2)
summary(info_cluster2)
```
It seems there are way more individuals with higher values of `cholesterol` and `gluc` in the second cluster. Furthermore, here seniors are the majority, while the first one consists mostly of elderly individuals.
```{r}
cluster_differences = c()
for (i in continuous_vars){
  wilcox_test_result <- wilcox.test(ap_hi ~ Cluster, data = cardio)
  cluster_differences[i] = wilcox_test_result$p.value
  print(wilcox_test_result$p.value)
}

for (i in categorical_vars){
  # Create a contingency table
  contingency_table <- table(cardio[[i]], cardio$Cluster)

  # Perform the Chi-Square test
  chi_square_result <- chisq.test(contingency_table)
  cluster_differences[i] = chi_square_result$p.value
  print(chi_square_result$p.value)
}

```
The clusters are very different across all metrics, not only those we supposed.
Summarizing our result, there seems to be a "healthier" group (cluster 1), where there are less people with high values of `cholesterol` and `gluc`, less drinkers, more active people, more elderly individuals and, finally, more females. 
Roughly 75% of cluster 1 doesn't have heart diseases, while 87 % of cluster 2 does.




### K-means clustering (only numerical)
```{r}
cardio_data_scaled <- scale(cardio_cl[, continuous_vars])

# View the first few rows of the dataset
head(cardio_data_scaled)
```

```{r}
# Compute the distance matrix for the scaled data
sample_cardio_data_scaled = sample(cardio_data_scaled, 10000)
dist_matrix <- dist(sample_cardio_data_scaled)
head(dist_matrix)
```


<<<<<<< HEAD
## Random Forest
=======
```{r}
# Perform K-means clustering
set.seed(123)  
silhouette_scores = c()

# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)
# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_clusters in 2:10) {
   # Perform k-means clustering
   kmeans_result <- kmeans(sample_cardio_data_scaled, centers = n_clusters)

   # Extract cluster labels from the k-means result
   cluster_labels <- kmeans_result$cluster

   # Calculate silhouette values
   sil <- silhouette(cluster_labels, dist_matrix)

   # Store the average silhouette score for the current number of clusters
   silhouette_scores[n_clusters] <- mean(sil[, 3])
 }

max_silhouette_kmeans = which.max(silhouette_scores)

best_kmeans_result <- kmeans(cardio_data_scaled, centers = max_silhouette_kmeans)
# Add the cluster assignment to the original data
cardio$Cluster <- as.factor(best_kmeans_result$cluster)
str(best_kmeans_result)
```

```{r}
# Visualize the clusters across different dimensions
cl_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
cl_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(cl_p1, cl_p2)
```
Plotting along the cardio and both types of pressures, the clusters seem to neatly separate the data along some pressure values. 
```{r}
cl_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(cl_p3, cardio_pl)
```
Let's test for mean differences across clusters
```{r}
info_cluster1 = cardio %>% filter(Cluster == 1)
summary(info_cluster1)
```
```{r}
info_cluster2 = cardio %>% filter(Cluster == 2)
summary(info_cluster2)
```


```{r}
cluster_differences = c()
for (i in continuous_vars){
  wilcox_test_result <- wilcox.test(ap_hi ~ Cluster, data = cardio)
  cluster_differences[i] = wilcox_test_result$p.value
  print(wilcox_test_result$p.value)
}

for (i in categorical_vars){
  # Create a contingency table
  contingency_table <- table(cardio[[i]], cardio$Cluster)

  # Perform the Chi-Square test
  chi_square_result <- chisq.test(contingency_table)
  cluster_differences[i] = chi_square_result$p.value
  print(chi_square_result$p.value)
}

```


### K-modes clustering (only categorical -> 8 clusters)
```{r}
library(klaR)
# Scale the data to normalize the values
# Create a vector of column names to exclude
columns_to_exclude <- c(continuous_vars, "cardio")

# Filter the dataset to exclude these columns and scale the remaining numeric columns
cardio_data_modes <- cardio_cl[, !colnames(cardio_cl) %in% columns_to_exclude]
# cardio_data_scaled <- scale(cardio_cl[, -12])

# View the first few rows of the dataset
head(cardio_data_modes)

# Perform K-modes clustering
set.seed(123)  
modes_cost = c()

# Compute the distance matrix for the scaled data
sample_cardio_data_modes = cardio_data_modes[1:1000, ]
#dist_matrix_modes <- dist(sample_cardio_data_modes, method = "binary")
dist_matrix_modes = daisy(sample_cardio_data_modes, metric = "gower")
# Try out different numbers of clustering and see which one has the highest silhouette score (meaning it best divides the data)

# Due to computational constraints we select just a subset of the original observations, 1/7, to tune this hyperparameter
 for (n_modes in 2:10) {
   # Perform k-means clustering
   kmodes_result <- kmodes(sample_cardio_data_modes, modes = n_modes)

   # Extract cluster labels from the k-means result
   modes_labels <- kmodes_result$modes
   
   # Store the average silhouette score for the current number of clusters
   modes_cost[n_modes] <-sum(kmodes_result$withindiff)
 }

min_cost_kmodes = which.min(modes_cost)

best_kmodes_result <- kmodes(cardio_data_modes, modes = max_silhouette_kmodes)
# Add the cluster assignment to the original data
cardio$Cluster_modes <- as.factor(best_kmodes_result$cluster)
str(best_kmodes_result)
# fviz_cluster(best_kmeans_result, data = cardio_data_scaled)
```
```{r}
# Visualize the clusters across different dimensions
modes_p1 = ggplot(cardio, aes(cardio, ap_hi, color = Cluster_modes)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_hi)")
modes_p2 = ggplot(cardio, aes(cardio, ap_lo, color = Cluster_modes)) +
  geom_point() +
  labs(title = "K-Means Clustering (cardio vs ap_lo)")

grid.arrange(modes_p1, modes_p2)
```
```{r}
modes_p3 = ggplot(cardio, aes(ap_hi, ap_lo, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
cardio_pl = ggplot(cardio, aes(ap_hi, ap_lo, color = cardio)) +
  geom_point() +
  labs(title = "K-Means Clustering (ap_hi vs ap_lo)")
grid.arrange(modes_p3, cardio_pl)
```

## Classification Trees
>>>>>>> 97a53dba6ef7f6c0c6051deb9c91b6910d31b131

### Classification trees

In order to correctly fit the random forest model, let's analyze first some simpler classifiers: classification trees. 

Firstly proposed by [@breiman1984classification], decision trees (or classification trees, when dealing with a discrete target variable) are unsupervised learning algorithms. The idea consists on recursively partition the feature space and assign a label to each resulting partition. The tree is then used to classify future points according to these splits and labels. The key advantage of decision trees over other methods is their high interpretability, a quality usually wanted when working in fields like healthcare [@bertsimas2017optimal].

The dataset will be split in two parts: 80% for training the model and 20% for validation. 
```{r classification trees (high cp) 1}
# Split data (train/test)
set.seed(123)
cardio_cl$cardio <- as.factor(cardio_cl$cardio)
train_index <- sample(1:nrow(cardio_cl), 0.8 * nrow(cardio_cl))  
train_data <- cardio_cl[train_index, ]
test_data <- cardio_cl[-train_index, ]
```

Now, it is time to fit the model. The function `rpart`, expects some user-defined parameters (the ones passed to the function below) but can also accept multiple optional parameters like `rpart.control`. This one controls some of the characteristic of the tree:

- `minsplit`: Minimum observations to attempt a split.
- `cp`: Complexity parameter for pruning.
- `maxdepth`: Maximum depth of the tree
- ...

```{r classification trees (high cp) 2}
# Build and plot the classification tree
tree <- rpart(cardio ~ ., data = train_data, method = "class")
tree$control
rpart.plot(tree)
```

At first sight, we can observe that the classification tree completely relies on the feature `ap_hi` for the only split. Let's check the importance of the different features in the model:

```{r classification trees (high cp) 3}
# Check feature importance
tree$variable.importance
```

It seem that `ap_hi`, together with `ap_lo`, out stand over the others features. Even though both of them are significant, they are highly correlated, so taking them into account at the same time would introduce redundancy in the tree.

In order to better understand the performance of this model, we can check its accuracy and the area under the ROC curve after trying to classify the testing data.

```{r classification trees (high cp) 4}
# Make predictions
predictions <- predict(tree, test_data, type = "prob")

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions[, 2] > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)
confusion_matrix

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# ROC curve
roc_curve <- roc(test_data$cardio, predictions[, 2])
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))
```

Could we improve the performance of the model with more splits? In order to force the tree to use less relevant features, we will readjust the complexity parameter (`cp`).

```{r classification trees (low cp) 1}
# New classification tree
tree <- rpart(cardio ~ ., data = train_data, method = "class",
              control = rpart.control(cp = 0.001))
rpart.plot(tree)
```

This time a much complex tree is created, taking into account all the features. All variables have now some importance in the model (even though the differences among them can be huge).

```{r classification trees (low cp) 2}
# Check feature importance
tree$variable.importance
```

Let's check the performance:

```{r classification trees (low cp) 3}
# Make predictions
predictions <- predict(tree, test_data, type = "prob")

# Convert probabilities to predicted classes
predicted_classes <- ifelse(predictions[, 2] > 0.5, 1, 0)

# Confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$cardio)
confusion_matrix

# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# ROC curve
roc_curve <- roc(test_data$cardio, predictions[, 2])
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# AUC
auc_value <- auc(roc_curve)
print(paste("AUC:", round(auc_value, 4)))
```

After evaluating both models, we get a sightly better accuracy from a significantly more complex tree. Moreover, the area under the ROC curve also increases (from 0.7056 to 0.7770). From this comparison, we can conclude that although the blood pressure is one the best indicators (in this dataset) of cardiovascular diseases, observing other variables like cholesterol, weight, age or glucose also helps to identify them.


### Random Forest model

Following classification trees, a natural approximation is fitting a random forest model. 

Introduced by [@breiman2001random], and explored by many other authors as [@hastie2009elements], Random Forest is an ensemble machine learning algorithm primarily used for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (for classification) or the mean/average prediction (for regression) of the individual trees. It is a robust and versatile method that can handle non-linear relationships, large datasets, and high-dimensional data with ease.

Since the trees that make up the model are completely independent and in order to speed up the training, the problem may be parallelized. Let's set it up:

```{r parellization}
# Detect the number of cores available on your machine and register them
cores <- parallel::detectCores()
cl <- makeCluster(cores)  # Create a cluster with the detected cores
registerDoParallel(cl)    # Register the parallel backend
```

For this model, instead of splitting the data in two as we previously did, we will apply a k-fold-cross-validation technique. As this is going to be one of the final classifiers, correctly evaluating its performance and fully using the dataset for both training and validation is of vital importance. 

Many parameters can be configured here. Some of them are:

- `trControl(folds)`: The number of folds for the cross validation.
- `tuneGrid(mtry)`: The number of features to use in each decision tree.
- `tuneGrid(splitrule)`: Rule for splitting the tree
- `tuneGrid(min.node.size)`: Minimum number of observations per leaf node.
- ...

After testing and analyzing all type of combination, the one which provided the best area under the ROC curve was the following one:

```{r random forest (training) 1}
# Cross-validation setup
train_control <- trainControl(
  method = "cv",          
  number = 5,           # Folds
  classProbs = TRUE,     # Enable probabilities
  summaryFunction = twoClassSummary
)

tune <- expand.grid(
    mtry = c(2),
    splitrule = c("gini"),
    min.node.size = c(1)
)
```

Finally, let's compute the random forest using our settings.

```{r random forest (training) 2}
# Factorize the variable if it is not a factor
if (!is.factor(cardio_cl$cardio)) {
  cardio_cl$cardio <- factor(cardio_cl$cardio, levels = c(0, 1), labels = c("No", "Yes"))
} else {
  # Refactor the variable with the correct levels
  if (!all(levels(cardio_cl$cardio) %in% c("No", "Yes"))) {
    cardio_cl$cardio <- factor(cardio_cl$cardio, levels = c(0, 1), labels = c("No", "Yes"))
  }
}

rf_model <- train(
  cardio ~ ., 
  data = cardio_cl,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune,
  metric = "ROC"
)

# Free resources
stopCluster(cl)

# Final model
print(rf_model)
```


### Performance

Regarding the performance of the final model:

```{r random forest (performance) 3}
# Get predicted probabilities
predictions <- rf_model$finalModel$predictions
predicted_probs <- predictions[, "Yes"] 
actual_labels <- cardio_cl$cardio 

# Manually compute the area under ROC curve
roc_curve <- roc(actual_labels, predicted_probs)
print(paste("AUC:", auc(roc_curve)))

# ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```

As we could imagine, this enhanced version of classification trees gets a better performance in the classification task. However, we must take into account the high computational complexity of this new model compared with the previous ones.


## References
