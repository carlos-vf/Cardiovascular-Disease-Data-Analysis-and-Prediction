rf_confusion_matrix
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[1,2]
FN <- cm$table[2,1]
TP <- cm$table[2,2]
# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)
# Create a dataframe for visualization
metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)
# Print computed metrics
print(metrics_df)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)
levels(cardio$cardio) <- c("No", "Yes")
levels(train_data$cardio) <- c("No", "Yes")
levels(test_data$cardio) <- c("No", "Yes")
# Cross-validation
log_model <- train(
cardio ~ .,                    # Formula
data = train_data,             # Dataset
method = "glm",                # Correct method for logistic regression
family = binomial(link = "logit"),  # Logistic regression
trControl = ctrl,              # Cross-validation settings
metric = "ROC"            # Appropriate metric for classification
)
# Print and summarize the model
print(log_model)
summary(log_model)
grid <- expand.grid(
ap_hi = seq(min(test_data$ap_hi), max(test_data$ap_hi), length.out = 200),
ap_lo = seq(min(test_data$ap_lo), max(test_data$ap_lo), length.out = 200)
)
# For other predictors, calculate medians for numerical variables and most frequent levels for factors
other_predictors <- test_data %>%
select(-c(ap_hi, ap_lo, cardio)) %>%
summarise(across(where(is.numeric), median), across(where(is.factor), ~ names(sort(table(.), decreasing = TRUE)[1])))
# Repeat median/frequent level values for all rows of the grid
grid <- cbind(grid, other_predictors[rep(1, nrow(grid)), ])
# Convert factor levels in the grid to match the cardio dataset's levels
grid <- grid %>%
mutate(across(where(is.factor), ~ factor(., levels = levels(cardio[[deparse(substitute(.))]]))))
# Predict probabilities for the grid
probabilities <- predict(log_model, newdata = grid, type = "prob")
grid$predicted_probs <- probabilities[, 2]  # Extract probabilities for the positive class
# Plot decision boundary
decision_boundary <- ggplot() +
# Decision boundary contour
geom_contour(data = grid, aes(x = ap_hi, y = ap_lo, z = predicted_probs), breaks = 0.5, color = "black", linetype = "solid", size = 1) +
# Filled contour plot
geom_tile(data = grid, aes(x = ap_hi, y = ap_lo, fill = predicted_probs), alpha = 0.8) +
scale_fill_gradient(low = "#FFCCCC", high = "#6699FF", name = "P(cardio=Yes)") +
# Original data points
geom_point(data = test_data, aes(x = ap_hi, y = ap_lo, color = as.factor(cardio)), size = 2.5) +
scale_color_manual(values = c("No" = "#33FF57", "Yes" = "#FF5733"), name = "Cardio") +
# Titles and themes
labs(
title = "Logistic Regression Decision Boundary",
x = "Systolic Blood Pressure (ap_hi)", y = "Diastolic Blood Pressure (ap_lo)"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold"),
legend.position = "right"
)
# Save the plot
ggsave("images/decision_boundary_cardio.png", decision_boundary, width = 8, height = 8)
# Display the plot
decision_boundary
# Get predicted probabilities
predictions <- predict(log_model, newdata = test_data, type = "prob")
predicted_probs <- predictions[, "Yes"]
actual_labels <- test_data$cardio
# Manually compute the area under ROC curve
roc_curve_lr <- roc(actual_labels, predicted_probs)
# ROC curve
plot(roc_curve_lr, main = "ROC Curve", col = "blue", lwd = 2)
# Save image
png("images/lr_roc_curve.png", width = 800, height = 600)
plot(roc_curve_lr, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
## Confusion Matrix for Logistic Regression
predicted_class <- ifelse(predicted_probs >= 0.5, "Yes", "No")
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))
# Convert the confusion matrix into a table
cm_table <- as.data.frame(cm$table)
colnames(cm_table) <- c("Prediction", "Reference", "Count")
# Plot the confusion matrix
lr_confusion_matrix <- ggplot(data = cm_table, aes(x = Prediction, y = Reference, fill = Count)) +
geom_tile() +
geom_text(aes(label = Count), color = "white") +
scale_fill_gradient(low = "blue", high = "red") +
theme_minimal() +
labs(title = "Confusion Matrix: Logistic Regression", x = "Prediction", y = "Actual")
ggsave("images/lr_confusion_matrix.png", lr_confusion_matrix, width = 8, height = 8)
lr_confusion_matrix
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[1,2]
FN <- cm$table[2,1]
TP <- cm$table[2,2]
# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)
# Create a dataframe for visualization
metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)
# Print computed metrics
print(metrics_df)
ratio <- metrics_df$Value[7]/metrics_df$Value[6]
## Confusion Matrix for Logistic Regression
predicted_class <- ifelse(predicted_probs >= 0.45, "Yes", "No")
cm <- confusionMatrix(as.factor(predicted_class), as.factor(actual_labels))
# Convert the confusion matrix into a table
cm_table <- as.data.frame(cm$table)
colnames(cm_table) <- c("Prediction", "Reference", "Count")
# Plot the confusion matrix
lr_confusion_matrix <- ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Count)) +
geom_tile() +
geom_text(aes(label = Count), color = "white") +
scale_fill_gradient(low = "blue", high = "red") +
theme_minimal() +
labs(title = "Confusion Matrix: Logistic Regression (threshold = 0.45)", x = "Actual", y = "Predicted")
ggsave("images/lr_confusion_matrix.png", lr_confusion_matrix, width = 8, height = 8)
lr_confusion_matrix
## Performance Metrics Calculation
TN <- cm$table[1,1]
FP <- cm$table[1,2]
FN <- cm$table[2,1]
TP <- cm$table[2,2]
# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)
f1_score <- 2 * ((precision * recall) / (precision + recall))
fpr <- FP / (FP + TN)
fnr <- FN / (FN + TP)
# Create a dataframe for visualization
metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity", "F1-Score", "FPR", "FNR"),
Value = c(accuracy, precision, recall, specificity, f1_score, fpr, fnr)
)
# Print computed metrics
print(metrics_df)
# Feature Importance
importance_lr <- as.data.frame(summary(log_model)$coefficients)
importance_lr$Feature <- rownames(importance_lr)
# Remove intercept for better visualization
importance_lr <- importance_lr[importance_lr$Feature != "(Intercept)", ]
# Rename columns for clarity
colnames(importance_lr) <- c("Estimate", "StdError", "ZValue", "PValue", "Feature")
# Plot feature importance
feature_importance <- ggplot(importance_lr, aes(x=reorder(Feature, Estimate), y=Estimate)) +
geom_bar(stat='identity', fill='steelblue') +
coord_flip() +
labs(title="Coefficient Value - Logistic Regression",
x="Feature", y="Value)") +
theme_minimal()
ggsave("images/feature_importance.png", feature_importance, width = 8, height = 8)
feature_importance
smoke_check = glm(cardio ~ smoke + alco, data = train_data, family = binomial(link = "logit") )
summary(smoke_check)
gluc_check = glm(cardio ~ gluc + cholesterol, data = train_data, family = binomial(link = "logit"))
summary(gluc_check)
interaction_gluc_cardio = glm(cardio ~ gluc*cholesterol, data = train_data, family = binomial(link = "logit"))
summary(interaction_gluc_cardio)
cardio_cl$ap_pu = cardio$ap_hi - cardio$ap_lo
log_model_ap_pu <- train(
cardio ~ . -ap_hi-ap_lo,                    # Formula
data = train_data,             # Dataset
method = "glm",                # Correct method for logistic regression
family = binomial(link = "logit"),  # Logistic regression
trControl = ctrl,              # Cross-validation settings
metric = "ROC"            # Appropriate metric for classification
)
print(log_model_ap_pu)
summary(log_model_ap_pu)
fit <- glm(cardio ~ ., data = cardio, family = binomial(link = "logit"))
pred <- predict(fit, type = "response")
res <- residuals(fit, type = "response")
cardio$cardio <- factor(cardio$cardio, levels = c("No", "Yes"), labels = c(0, 1))
pearson_residuals <- (as.numeric(as.character(cardio$cardio)) - fit$fitted.values)/sqrt(fit$fitted.values*(1 - fit$fitted.values))
plot(cardio$ap_hi, pearson_residuals)
binnedplot(pred,res, nclass = 1000)
fit_poly3 <- glm(cardio ~ .+poly(ap_hi, 3), data = cardio, family = binomial(link = "logit"))
pred <- predict(fit_poly3, type = "response")
res <- residuals(fit_poly3, type = "response")
summary(fit_poly3)
pearson_residuals <- (as.numeric(as.character(cardio$cardio)) - fit$fitted.values)/sqrt(fit_poly3$fitted.values*(1 - fit_poly3$fitted.values))
plot(cardio$ap_hi, pearson_residuals, ylim = c(-10,10))
binnedplot(pred,res, nclass = 1000)
fit_poly4 <- glm(cardio ~ .+poly(ap_hi, 4), data = cardio, family = binomial(link = "logit"))
pred <- predict(fit_poly4, type = "response")
res <- residuals(fit_poly4, type = "response")
summary(fit_poly4)
pearson_residuals <- (as.numeric(as.character(cardio$cardio)) - fit_poly4$fitted.values)/sqrt(fit_poly4$fitted.values*(1 - fit_poly4$fitted.values))
plot(cardio$ap_hi, pearson_residuals, ylim = c(-10,10))
binnedplot(pred,res, nclass = 1000)
br <- arm::binned.resids(pred,res)
set.seed(23)
# Model matrix creation
train_data_dummies <- model.matrix(cardio ~ ., data = train_data)[, -1]  # Exclude intercept column
test_data_dummies <- model.matrix(cardio ~ ., data = test_data)[, -1]    # Exclude intercept column
X_train <- scale(train_data_dummies)
Y_train <- as.numeric(train_data$cardio) - 1  # Assuming 'cardio' is a factor variable (0/1)
X_test <- scale(test_data_dummies)
Y_test <- as.numeric(test_data$cardio) - 1  # Assuming 'cardio' is a factor variable (0/1)
# Fit the model using cv.glmnet
cvfit <- cv.glmnet(X_train, Y_train, family = "binomial", type.measure = "class", alpha = 1)
plot(cvfit)
print(cvfit)
# Get optimal lambda values
lambda_min <- cvfit$lambda.min
lambda_1se <- cvfit$lambda.1se
cat("Lambda.min:", lambda_min, "\n")
cat("Lambda.1se:", lambda_1se, "\n")
# Coefficients
cat("Coefficients at lambda.min:\n")
print(coef(cvfit, s = "lambda.min"))
cat("Coefficients at lambda.1se:\n")
print(coef(cvfit, s = "lambda.1se"))
# Predictions on test data
predictions <- predict(cvfit, newx = X_test, s = "lambda.min", type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Training predictions (lambda.min)
train_predictions_min <- predict(cvfit, newx = X_train, s = "lambda.min", type = "response")
train_pred_classes_min <- ifelse(train_predictions_min > 0.5, 1, 0)
train_accuracy_min <- mean(train_pred_classes_min == Y_train)
train_roc_min <- roc(Y_train, as.vector(train_predictions_min))
train_conf_matrix_min <- table(Predicted = train_pred_classes_min, Actual = Y_train)
# Test predictions (lambda.min)
test_predictions_min <- predict(cvfit, newx = X_test, s = "lambda.min", type = "response")
test_pred_classes_min <- ifelse(test_predictions_min > 0.5, 1, 0)
test_accuracy_min <- mean(test_pred_classes_min == as.numeric(test_data$cardio) - 1)
test_roc_min <- roc(as.numeric(test_data$cardio) - 1, as.vector(test_predictions_min))
test_conf_matrix_min <- table(Predicted = test_pred_classes_min, Actual = as.numeric(test_data$cardio) - 1)
# Training predictions (lambda.1se)
train_predictions_1se <- predict(cvfit, newx = X_train, s = "lambda.1se", type = "response")
train_pred_classes_1se <- ifelse(train_predictions_1se > 0.5, 1, 0)
train_accuracy_1se <- mean(train_pred_classes_1se == Y_train)
train_roc_1se <- roc(Y_train, as.vector(train_predictions_1se))
train_conf_matrix_1se <- table(Predicted = train_pred_classes_1se, Actual = Y_train)
# Test predictions (lambda.1se)
test_predictions_1se <- predict(cvfit, newx = X_test, s = "lambda.1se", type = "response")
test_pred_classes_1se <- ifelse(test_predictions_1se > 0.5, 1, 0)
test_accuracy_1se <- mean(test_pred_classes_1se == as.numeric(test_data$cardio) - 1)
test_roc_1se <- roc(as.numeric(test_data$cardio) - 1, as.vector(test_predictions_1se))
test_conf_matrix_1se <- table(Predicted = test_pred_classes_1se, Actual = as.numeric(test_data$cardio) - 1)
# Calculate MSE for training and test sets
train_mse_min <- mean((train_predictions_min - Y_train)^2)
train_mse_1se <- mean((train_predictions_1se - Y_train)^2)
test_mse_min <- mean((test_predictions_min - (as.numeric(test_data$cardio) - 1))^2)
test_mse_1se <- mean((test_predictions_1se - (as.numeric(test_data$cardio) - 1))^2)
# Create a summary table
results <- data.frame(
Metric = c(
"Training Accuracy", "Training AUC", "Training MSE",
"Test Accuracy", "Test AUC", "Test MSE"
),
Lambda_Min = c(
train_accuracy_min, auc(train_roc_min), train_mse_min,
test_accuracy_min, auc(test_roc_min), test_mse_min
),
Lambda_1SE = c(
train_accuracy_1se, auc(train_roc_1se), train_mse_1se,
test_accuracy_1se, auc(test_roc_1se), test_mse_1se
)
)
print("Summary of Results:")
results
fits1 <- glmnet(X_train, Y_train, family = "binomial", type.measure = "class", alpha = 1, lambda = lambda_1se)
# Get predicted probabilities
predicted_probs <- predict(fits1, newx = X_test, type = "response")
predicted_class <- ifelse(predicted_probs >= 0.5, "Yes", "No")
actual_labels <- as.factor(Y_test)
levels(actual_labels) <- c("No", "Yes")
# Create confusion matrix
conf_matrix <- confusionMatrix(
factor(predicted_class),
factor(actual_labels),
positive = "Yes"
)
lasso_sensitivity <- conf_matrix$byClass["Sensitivity"]
lasso_specificity <- conf_matrix$byClass["Specificity"]
# Manually compute the area under ROC curve
roc_curve_lasso <- roc(actual_labels, as.numeric(predictions))
# ROC curve
plot(roc_curve_lasso, main = "ROC Curve for LASSO", col = "blue", lwd = 2)
roc_curve_lasso$auc
# Cross-validation
poly3 <- train(
cardio ~ . + poly(ap_hi, 3),                    # Formula
data = train_data,             # Dataset
method = "glm",                # Correct method for logistic regression
family = binomial(link = "logit"),  # Logistic regression
trControl = ctrl,              # Cross-validation settings
metric = "ROC"            # Appropriate metric for classification
)
# General results
poly3$results
# Get predicted probabilities
predictions <- predict(poly3, newdata = test_data, type = "prob")
predicted_probs <- predictions[, "Yes"]
actual_labels <- test_data$cardio
# Manually compute the area under ROC curve
roc_curve_poly3 <- roc(actual_labels, predicted_probs)
# ROC curve
plot(roc_curve_poly3, main = "ROC Curve for poly3", col = "blue", lwd = 2)
roc_curve_poly3$auc
# Cross-validation
poly4 <- train(
cardio ~ . + poly(ap_hi, 4),                    # Formula
data = train_data,             # Dataset
method = "glm",                # Correct method for logistic regression
family = binomial(link = "logit"),  # Logistic regression
trControl = ctrl,              # Cross-validation settings
metric = "ROC"            # Appropriate metric for classification
)
# General results
poly4$results
# Get predicted probabilities
predictions <- predict(poly4, newdata = test_data, type = "prob")
predicted_probs <- predictions[, "Yes"]
actual_labels <- test_data$cardio
# Manually compute the area under ROC curve
roc_curve_poly4 <- roc(actual_labels, predicted_probs)
# ROC curve
plot(roc_curve_poly4, main = "ROC Curve for poly4", col = "blue", lwd = 2)
roc_curve_poly4$auc
# Plot the ROC curves
roc_results <- plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
roc_results <- plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
# Plot the ROC curves
roc_results <- plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
roc_results <- plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
roc_results <- plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
roc_results <- plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
roc_results <- plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
# Add a legend
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4"), col = c("red", "blue", "pink", "green", "orange"), lwd = 2)
# ROC curve
roc_results
# Save image
#png("images/rf_roc_curve.png", width = 800, height = 600)
#plot(roc_curve_rf, main = "ROC Curve", col = "blue", lwd = 2)
#dev.off()
# Model results
results <- data.frame(
Model = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3", "Polynomial Regression 4"),
AUC = c(
rf_model$results$ROC,
log_model$results$ROC,
roc_curve_lasso$auc,
poly3$results$ROC,
poly4$results$ROC
),
Sensitivity = c(
rf_model$results$Sens,
log_model$results$Sens,
lasso_sensitivity,
poly3$results$Sens,
poly4$results$Sens
),
Specificity = c(
rf_model$results$Spec,
log_model$results$Spec,
lasso_specificity,
poly3$results$Spec,
poly4$results$Spec
)
)
print(results)
# Plot the ROC curves
roc_results <- plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
roc_results <- plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
roc_results <- plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
roc_results <- plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
roc_results <- plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
# Add a legend
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4"), col = c("red", "blue", "pink", "green", "orange"), lwd = 2)
# ROC curve
roc_results
# Save image
png("images/roc_results.png", width = 800, height = 600)
plot(roc_results, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
# Model results
results <- data.frame(
Model = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3", "Polynomial Regression 4"),
AUC = c(
rf_model$results$ROC,
log_model$results$ROC,
roc_curve_lasso$auc,
poly3$results$ROC,
poly4$results$ROC
),
Sensitivity = c(
rf_model$results$Sens,
log_model$results$Sens,
lasso_sensitivity,
poly3$results$Sens,
poly4$results$Sens
),
Specificity = c(
rf_model$results$Spec,
log_model$results$Spec,
lasso_specificity,
poly3$results$Spec,
poly4$results$Spec
)
)
print(results)
# Plot the ROC curves
roc_results <- plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
# Add a legend
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4"), col = c("red", "blue", "pink", "green", "orange"), lwd = 2)
# ROC curve
roc_results
# Save image
png("images/roc_results.png", width = 800, height = 600)
plot(roc_results, main = "ROC Curve", col = "blue", lwd = 2)
dev.off()
# Model results
results <- data.frame(
Model = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3", "Polynomial Regression 4"),
AUC = c(
rf_model$results$ROC,
log_model$results$ROC,
roc_curve_lasso$auc,
poly3$results$ROC,
poly4$results$ROC
),
Sensitivity = c(
rf_model$results$Sens,
log_model$results$Sens,
lasso_sensitivity,
poly3$results$Sens,
poly4$results$Sens
),
Specificity = c(
rf_model$results$Spec,
log_model$results$Spec,
lasso_specificity,
poly3$results$Spec,
poly4$results$Spec
)
)
print(results)
# Plot the ROC curves
plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
# Add a legend
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4"), col = c("red", "blue", "pink", "green", "orange"), lwd = 2)
# Save image
png("images/roc_results.png", width = 800, height = 600)
plot(roc_curve_rf, col = "red", lwd = 2, main = "ROC Curves for the main models")
plot(roc_curve_lr, col = "blue", lwd = 2, add = TRUE)
plot(roc_curve_lasso, col = "pink", lwd = 2, add = TRUE)
plot(roc_curve_poly3, col = "green", lwd = 2, add = TRUE)
plot(roc_curve_poly4, col = "orange", lwd = 2, add = TRUE)
legend("bottomright", legend = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3",  "Polynomial Regression 4"), col = c("red", "blue", "pink", "green", "orange"), lwd = 2)
dev.off()
# Model results
results <- data.frame(
Model = c("Random Forest", "Basic Logistic Regression", "LASSO", "Polynomial Regression 3", "Polynomial Regression 4"),
AUC = c(
rf_model$results$ROC,
log_model$results$ROC,
roc_curve_lasso$auc,
poly3$results$ROC,
poly4$results$ROC
),
Sensitivity = c(
rf_model$results$Sens,
log_model$results$Sens,
lasso_sensitivity,
poly3$results$Sens,
poly4$results$Sens
),
Specificity = c(
rf_model$results$Spec,
log_model$results$Spec,
lasso_specificity,
poly3$results$Spec,
poly4$results$Spec
)
)
print(results)
